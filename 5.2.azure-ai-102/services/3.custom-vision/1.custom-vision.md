# ðŸ§  Azure Custom Vision Service â€“ Image Classification & Object Detection

## ðŸ“˜ Official Definition

**Azure Custom Vision** is part of Azure Cognitive Services that allows you to build, deploy, and improve image classifiers using your own custom data. Unlike pre-trained models, you train it with your own labeled images to classify specific objects (Image Classification) or identify multiple objects within a single image (Object Detection).

---

## ðŸŽ¯ What You Can Do with Custom Vision

| Feature                  | Purpose                                                                             |
| ------------------------ | ----------------------------------------------------------------------------------- |
| **Image Classification** | Tag an image as belonging to one or more categories (e.g., "Hemlock" vs "Cherry").  |
| **Object Detection**     | Detect multiple objects within a single image and locate them using bounding boxes. |

---

## ðŸ§¬ How Custom Vision Works (Conceptually)

```mermaid
flowchart TD
    A[Upload Labeled Images] --> B[Train the Model]
    B --> C[Adjust Weights & Biases Internally]
    C --> D[Create Training Iteration]
    D --> E[Publish Iteration to Endpoint]
    E --> F[Use Endpoint to Classify or Detect Objects in New Images]
```

- ðŸ§  A **model** is made of **weights and biases** connecting neurons.
- ðŸ” Training updates those weights using algorithms like **backpropagation** to minimize **cost functions**.
- ðŸ·ï¸ Each image must be **tagged accurately**. Bad tagging = corrupt model = AI hallucinations.
- ðŸ§¼ Clean + accurate + sufficient data is ðŸ”‘.

---

## ðŸ› ï¸ Custom Vision Workflow â€“ Step-by-Step

```mermaid
sequenceDiagram
    participant You
    participant Azure Portal
    participant Python SDK
    participant Trained Model

    You->>Azure Portal: Create Custom Vision Resource (Training + Prediction)
    You->>Python SDK: Upload tagged images (Hemlock, Cherry, etc.)
    Python SDK->>Azure Portal: Store images
    You->>Python SDK: Trigger training
    Python SDK->>Trained Model: Train model with your images
    Trained Model-->>Python SDK: Iteration ready!
    You->>Python SDK: Publish iteration
    Python SDK->>Azure Portal: Deploy iteration to prediction endpoint
    You->>Prediction Endpoint: Send new image
    Prediction Endpoint-->>You: Return tag + confidence
```

---

## ðŸ–¼ï¸ Hands-on Example â€“ Image Classification

ðŸª´ Tags used: `Hemlock`, `Japanese Cherry`  
ðŸ“ Data: 10 images per tag

ðŸ“Œ Key endpoints:

- **Training endpoint**: Used to create and train models
- **Prediction endpoint**: Used to make predictions with trained models

### Example Use Case

> Upload multiple images of Hemlock and Japanese Cherry, train a model, and then classify a new image as either tag.

### Model Precision Metrics

| Metric        | Meaning                                                             |
| ------------- | ------------------------------------------------------------------- |
| **Precision** | How often a predicted tag is correct                                |
| **Recall**    | Out of all correct tags, how many were predicted correctly          |
| **AP**        | Average Precision â€“ combo of precision and recall across thresholds |

Example output after training:

- `Hemlock` â†’ 99.99% confidence
- `Cherry` â†’ 0.000003% confidence (basically none)

ðŸ§  Fun Fact: AI never guarantees 100% certainty. Itâ€™s always **"inference"**, not prediction, and neural networks inherently allow some margin of error.

---

## ðŸ” Object Detection (ðŸ—‚ï¸ Tags + ðŸ“¦ Bounding Boxes)

ðŸ“Œ Use case: Detect **multiple objects in one image**, like both a `fork` and a `scissor`.

### Key Difference from Classification

- You must define **regions of interest** (bounding boxes) for each object inside the image.
- Images and corresponding box coordinates are used for training.

### Output Example

- ðŸ“· New image with a fork
- Output â†’ â€œForkâ€ detected with 81% confidence, bounding box shown

---

## ðŸ’» SDK Behavior Summary

| Component                            | Role                                                         |
| ------------------------------------ | ------------------------------------------------------------ |
| `CustomVisionTrainingClient`         | Creates projects, tags, uploads images, starts training      |
| `CustomVisionPredictionClient`       | Calls the prediction endpoint with a new image               |
| `trainer.create_project()`           | Initializes a project to host image classification/detection |
| `trainer.create_tag()`               | Labels/tag for your object                                   |
| `trainer.create_images_from_files()` | Uploads images along with tags to train                      |
| `trainer.train_project()`            | Starts training                                              |
| `trainer.publish_iteration()`        | Publishes model to endpoint                                  |
| `predictor.classify_image()`         | Runs the model on new image (classification)                 |
| `predictor.detect_image()`           | Detects multiple objects in one image                        |

---

## ðŸ§‘â€ðŸ’» Python Tips & LLM Limitation

âš™ï¸ Debugging SDK? Use the `function` variable in IDE to explore available methods if docs are unclear.

â— At the time of writing:

> LLMs (even ChatGPT, GitHub Copilot) were **unable to generate working code for advanced Custom Vision SDK flows**. Manual exploration + documentation was still necessary.

---

## ðŸ§ª Studio vs Code

| Feature              | Azure Vision Studio         | Python SDK                       |
| -------------------- | --------------------------- | -------------------------------- |
| Upload images        | Drag & drop UI              | Programmatically                 |
| Create tags/projects | GUI                         | SDK                              |
| See metrics          | Graphs for precision/recall | Must be handled manually/logging |
| Predict new images   | Upload and click            | Predict via code or API call     |
| Annotate objects     | Draw bounding boxes on UI   | Provide coordinates via code     |

---

## ðŸ§¼ Best Practices

- âœ… **Use clean, correctly tagged data**
- âœ… **Use similar images for better training**
- âœ… **Avoid ambiguous or mislabeled samples**
- âœ… **Tag each image with multiple relevant tags if needed**
- âœ… **Use Azure Vision Studio for visual inspection & debugging**

---

## ðŸ§  Summary

| Concept              | Description                                       |
| -------------------- | ------------------------------------------------- |
| Model                | A set of weights & biases that recognize patterns |
| Training             | Updates the model with your new data              |
| Inference            | AIâ€™s prediction when shown new data               |
| Image Classification | One or more tags per image                        |
| Object Detection     | Locate multiple objects in an image               |
| Vision Studio        | GUI for uploading, training, and analyzing models |
