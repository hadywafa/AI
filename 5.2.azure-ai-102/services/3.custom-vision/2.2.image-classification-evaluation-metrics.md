# üß™ Image Classification Evaluation Metrics ‚Äì Masterclass üéØ

Whether you're classifying **cats vs dogs**, or **pineapple vs pizza üçï**, you need to know _how well_ your classifier performs.

> These metrics tell you **how often your AI is right**, how often it messes up, and **what kind of mistakes it makes**.

Let‚Äôs go from **the basics ‚û°Ô∏è all the way to advanced mAP and multi-class logic**, without fluff.

---

## üß† What Are We Measuring?

Your model predicts:

- ‚ÄúThis is a cat‚Äù
- ‚ÄúThis is a hotdog‚Äù
- ‚ÄúThis is _not_ a hotdog‚Äù

But‚Ä¶ was it _correct_? Let‚Äôs see how we score it üî¢

---

## ‚öñÔ∏è Key Metrics (And What They Mean)

| Metric               | Definition                                                             |
| -------------------- | ---------------------------------------------------------------------- |
| **Accuracy**         | % of total predictions that were correct (basic score)                 |
| **Precision**        | Of the items predicted as ‚Äúclass X‚Äù, how many were truly class X?      |
| **Recall**           | Of all items that actually are ‚Äúclass X‚Äù, how many did the model find? |
| **F1 Score**         | The harmonic mean of Precision and Recall (balance metric)             |
| **Confusion Matrix** | Table that shows **what got confused with what** (super useful)        |
| **mAP (optional)**   | If you enable probability thresholds, you can still compute mAP        |

---

## üç© Sweet Analogy: Classifying Donuts

You‚Äôre building an AI to detect üç© donuts vs ü•Ø bagels vs ü•ê croissants.

Your AI says:

- üç© ‚Üí Correct? ‚úÖ
- ü•ê ‚Üí Thought it was a bagel ‚ùå
- ü•Ø ‚Üí Missed it entirely ‚ùå

How do we score it?

---

## ‚úÖ Accuracy

> **"How often is my model correct?"**

```text
Accuracy = (TP + TN) / Total Predictions
```

üìå Problem: Doesn‚Äôt tell you which class is doing poorly!

‚úÖ Use **only** when classes are balanced
üö´ Don‚Äôt trust it when classes are imbalanced

---

## üéØ Precision & üîç Recall

Let‚Äôs say you‚Äôre focused on just the ü•ê class:

- **TP (True Positive):** ü•ê correctly predicted
- **FP (False Positive):** Not ü•ê, but predicted as ü•ê
- **FN (False Negative):** Actually ü•ê, but missed it

### üéØ Precision

```text
Precision = TP / (TP + FP)
```

‚û°Ô∏è _Out of all predicted ü•ê, how many were correct?_

### üîç Recall

```text
Recall = TP / (TP + FN)
```

‚û°Ô∏è _Out of all real ü•ê in the test set, how many did we find?_

---

### üìè F1 Score

> One number to rule them all ‚Äì balances precision vs recall

```text
F1 = 2 * (Precision * Recall) / (Precision + Recall)
```

üìå Great for **imbalanced datasets** (e.g. 90% cats, 10% dogs)

---

## ü§Ø Confusion Matrix

Here‚Äôs what your model really saw:

| Actual / Predicted | üç© Donut | ü•Ø Bagel | ü•ê Croissant |
| ------------------ | -------- | -------- | ------------ |
| üç© Donut           | **15**   | 2        | 1            |
| ü•Ø Bagel           | 1        | **12**   | 3            |
| ü•ê Croissant       | 0        | 2        | **11**       |

üß† This shows:

- It sometimes thinks **ü•ê = ü•Ø**
- It‚Äôs very good at detecting üç©

Use it to **diagnose weaknesses** in your model.

---

## üß† Macro vs Micro vs Weighted Averages

If you‚Äôre working with **multi-class classification** (like above), Azure and AI-102 expect you to know this:

| Average Type | What It Means                                                              |
| ------------ | -------------------------------------------------------------------------- |
| **Micro**    | Global precision/recall across all classes (treats all equally)            |
| **Macro**    | Average of metrics per class (treats all classes equally)                  |
| **Weighted** | Average per class, _weighted by class size_ (accounts for class imbalance) |

---

## üì¶ Azure Custom Vision: How It Shows Metrics

After training a model:

- You'll see **per-tag Precision, Recall, and AP**
- Also a **macro-averaged F1 score**
- If you export the model performance, you get a CSV with:

  - **TagName**
  - **Precision**
  - **Recall**
  - **True Positives, False Positives, False Negatives**

üìå For **multiclass mode**, only **one tag is predicted per image**
üìå For **multilabel mode**, **multiple tags** can be assigned per image

---

## üß™ Visual Summary

```mermaid
graph TD
    A[Model Prediction] --> B{Correct?}
    B -- Yes --> C[True Positive]
    B -- No --> D{Actual Class?}
    D -- Positive --> E[False Negative]
    D -- Other --> F[False Positive]
```

---

## üß† When to Use What?

| Goal                             | Use This Metric          |
| -------------------------------- | ------------------------ |
| Want an overall score            | Accuracy                 |
| Want fewer false alarms          | Precision                |
| Want to catch every real example | Recall                   |
| Want a good balance              | F1 Score                 |
| Need per-class insights          | Confusion Matrix         |
| Have multiple labels per image   | Precision/Recall per tag |
| AI-102 exam?                     | **ALL of the above üòÑ**  |

---

## üìå AI-102 Exam Tips

- Know the difference between Precision & Recall
- Understand confusion matrix for multiclass
- F1 = 2 \* P \* R / (P + R)
- Azure Custom Vision provides per-tag metrics
- Micro vs Macro averages are **often asked**

---

## üîÅ Example Recap

Let‚Äôs say for class ‚ÄúDog‚Äù:

- TP = 40
- FP = 10
- FN = 5

```text
Precision = 40 / (40 + 10) = 0.80
Recall    = 40 / (40 + 5) = 0.89
F1        = 2 * (0.8 * 0.89) / (0.8 + 0.89) ‚âà 0.84
```

---

## ‚úÖ Summary Cheat Sheet

| Term             | Formula                  | Measures                  |
| ---------------- | ------------------------ | ------------------------- |
| Accuracy         | (TP + TN) / Total        | Overall correctness       |
| Precision        | TP / (TP + FP)           | How exact the model is    |
| Recall           | TP / (TP + FN)           | How complete the model is |
| F1 Score         | 2PR / (P + R)            | Balanced metric           |
| Confusion Matrix | Table of TP, FP, FN, etc | Class-specific analysis   |
