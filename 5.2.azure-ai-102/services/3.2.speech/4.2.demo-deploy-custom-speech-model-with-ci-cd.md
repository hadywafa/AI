# 🚀 Deploying a Custom Speech Model using CI/CD

Welcome to the most **epic showdown** between "Manual Click-Ops" and "Automated Deployment Glory." In this topic, you’ll learn how to **deploy a trained Azure Custom Speech Model using CI/CD pipelines** in GitHub Actions. We’ll walk through everything from repo setup to automating training, testing, and deployment — with a real, working example.

---

## 🧠 What’s This About?

We previously learned how to train a custom speech model in **Azure Speech Studio** using dinosaur names (yep 🦕). Now, let’s automate it using **MLOps concepts** and GitHub Actions so that it:

- 🎯 Tests model quality
- 🧪 Trains the model
- ✅ Compares old vs new accuracy
- 📦 Deploys only if the new model is better
- 🔁 Can be reused for future speech projects (like product names, cities, medical terms)

---

## 🔁 CI/CD Pipeline Summary (CI vs CD)

| Step   | CI (Continuous Integration)     | CD (Continuous Deployment)                          |
| ------ | ------------------------------- | --------------------------------------------------- |
| Goal   | Test accuracy of baseline model | Train new model, test it, and deploy if it's better |
| Input  | Test data (audio + transcript)  | Training data (e.g. list of words), test data       |
| Output | Error rate (e.g. 20%)           | Deployment of improved model (e.g. 3% error)        |

---

## 🗃️ Folder Structure in the GitHub Repo

GitHub Repo: [Azure-Samples/Speech-Service-Actions-Template](https://github.com/Azure-Samples/Speech-Service-Actions-Template)

```ini
📁 .github/workflows/
│   ├── speech-test-data-ci.yml    # CI - benchmark testing
│   └── speech-train-deploy-cd.yml # CD - training and deploying
📁 test-data/
│   └── test/                      # Contains audio + transcript (benchmark)
📁 training-data/
│   ├── language.txt               # Words to teach (e.g. "GoDrive", "Sharjah")
│   └── pronunciation.txt          # Optional phonetic mappings
```

---

## 🛠️ CI Pipeline: speech-test-data-ci.yml

This pipeline calculates how “bad” the **baseline model** is. Think of it like a school test:

- 🎧 Give the model an audio file ("GoDrive")
- 📄 Compare its transcription with a correct one
- 📊 Measure word error rate (WER) — e.g. 19.6%
- 💾 Save this benchmark in Azure Blob Storage

### 🧩 Key Steps:

- **Login to Azure**
- **Upload test data (audio + transcript)**
- **Run `spx csr evaluate` command via CLI**
- **Save results in Blob Storage**

> 📌 This pipeline does not train the model — it only evaluates the default one.

---

## 🚀 CD Pipeline: speech-train-deploy-cd.yml

Now let’s _train_, _test_, and _deploy_ if the model is better.

### 🧩 Key Steps:

1. **Login to Azure**
2. **Upload training data** (`language.txt`, `pronunciation.txt`)
3. **Train the model** using:

   ```bash
   spx csr model create --name "newModel"
   ```

4. **Test the model** using same test data:

   ```bash
   spx csr evaluation create --model-id newModel
   ```

5. **Compare the WER** with previous benchmark
6. **If better (e.g., 1.7% vs 19.6%)**, deploy model using:

   ```bash
   spx csr endpoint create --model-id newModel
   ```

---

## 🔐 Secrets & Setup in GitHub Actions

You need the following GitHub Secrets to authenticate and trigger pipelines:

| Secret Name             | Purpose                             |
| ----------------------- | ----------------------------------- |
| `AZURE_CREDENTIALS`     | Service principal JSON for login    |
| `SPEECH_KEY`            | Azure Speech API key                |
| `SPEECH_REGION`         | Azure region (e.g., `eastus`)       |
| `AZURE_STORAGE_ACCOUNT` | Azure Blob Storage for test results |
| `PROJECT_NAME`          | Name of your Speech project         |

> These are used by the pipeline to access Azure, Speech Studio, and storage securely.

---

## 🧠 How It Knows What to Do?

Both workflows use a CLI called **Speech CLI (SCLI)** which mimics all the clicks you’d do in Speech Studio. It's called via this command:

```bash
spx csr model create --project-name "YourProject" --language-en "language.txt"
```

And evaluated using:

```bash
spx csr evaluation create --model-id your_model_id --dataset "test_dataset"
```

---

## 🔬 Sample Result (From Blob)

| Model      | WER (Word Error Rate)   |
| ---------- | ----------------------- |
| Baseline   | 19.6% (before training) |
| Trained V2 | 1.7% ✅                 |

📂 Stored in Azure Blob Storage for future comparison and tracking.

---

## 🧪 When Will It Run?

- ✅ **CI runs** when you push a **Git tag** like `baseline-1`
- ✅ **CD runs** when you **trigger it manually** (or you can automate it via PRs or commits)

---

## 👀 GitHub Logs: Peek Inside

You can inspect each GitHub Actions step live:

1. Go to your repo → **Actions tab**
2. Click on the workflow → **Expand steps**
3. See logs, outputs, even SPX commands that ran

Example:

```bash
spx csr evaluation create --model-id model123 --dataset testData123
```

---

## 🧼 Optional Cleanup (or Not)

You can optionally **delete models and data** at the end of the pipeline to save cost. Or **comment out** cleanup steps in shell script like:

```bash
spx csr evaluation delete --evaluation-id ...
```

> ⚠️ Speech training is **expensive** — plan for \~\$50–\$100 per run on Standard tier.

---

## 💡 Summary: What You Just Automated

- ✅ Train custom speech model (from CLI)
- ✅ Evaluate both old & new model
- ✅ Compare results programmatically
- ✅ Deploy only when model improves
- ✅ Store everything in Azure for visibility
