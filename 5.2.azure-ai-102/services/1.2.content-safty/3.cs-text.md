# ðŸ’¬ Azure AI Content Safety â€“ Text Analysis with Python

## ðŸ§¨ Why Text Moderation Is a Must

Whether itâ€™s trolls, toxicity, or just plain terrible DMs, **text content** is where the internet gets spicyâ€¦ and not always in a good way ðŸŒ¶ï¸. If you're building a chat app, a comment system, or user feedback platform, **you need protection** against:

- Hate speech ðŸ§¨
- Suicide or self-harm mentions ðŸ©¸
- Sexual content ðŸ‘
- Violent threats âš”ï¸

Thatâ€™s where **Azure AI Content Safety** steps in â€” your AI-powered content bouncer ðŸ§ ðŸ‘®.

---

## ðŸ§¾ Official Overview

> **Azure AI Content Safety** helps detect and classify harmful user-generated text across **4 categories**, returning severity scores (0 to 6). Itâ€™s built for **real-time moderation**, **policy enforcement**, and **keeping your platform safe**.

### Categories it detects

| Category     | Description                                 |
| ------------ | ------------------------------------------- |
| ðŸ§¨ Hate      | Racist, discriminatory, or abusive language |
| ðŸ©¸ Self-Harm | Mentions of suicide, self-injury            |
| ðŸ‘ Sexual    | Inappropriate, adult, or suggestive content |
| âš”ï¸ Violence  | Threats, gore, violent expressions          |

---

## ðŸ§° Prerequisites

1. ðŸ§  Azure Subscription
2. â˜ï¸ Provision **Azure AI Content Safety** resource
3. ðŸ”‘ Get `endpoint` and `key` from Azure Portal
4. ðŸ› ï¸ Install SDK

```bash
pip install azure-ai-contentsafety
```

---

## ðŸ§ª Full Code Example: Analyzing Text for Bad Vibes

Hereâ€™s a full working example using the official `azure-ai-contentsafety` SDK â€” fully explained below.

### ðŸ§¾ Python Script

```python
# Azure AI ContentSafety Text Analysis Example
# pip install azure-ai-contentsafety

import os
from azure.ai.contentsafety import ContentSafetyClient
from azure.ai.contentsafety.models import TextCategory, AnalyzeTextOptions
from azure.core.credentials import AzureKeyCredential
from azure.core.exceptions import HttpResponseError

# ðŸ‘® Replace with your real keys
key = "f7324ffadc7e4da0b5b253230a44a849"
endpoint = "https://azure-content-safety-demo-345.cognitiveservices.azure.com/"

# ðŸ”§ Initialize the Content Safety client
client = ContentSafetyClient(endpoint, AzureKeyCredential(key))

# ðŸ—¯ï¸ Suspicious text to analyze
request = AnalyzeTextOptions(text="You are an idiot. I will kill you.")

# ðŸ§  Call the API to analyze text
try:
    response = client.analyze_text(request)
except HttpResponseError as e:
    print("Analyze text failed.")
    if e.error:
        print(f"Error code: {e.error.code}")
        print(f"Error message: {e.error.message}")
    raise

# ðŸ“Š Extract and display results
hate_result = next(
    item for item in response.categories_analysis
    if item.category == TextCategory.HATE
)
self_harm_result = next(
    item for item in response.categories_analysis
    if item.category == TextCategory.SELF_HARM
)
sexual_result = next(
    item for item in response.categories_analysis
    if item.category == TextCategory.SEXUAL
)
violence_result = next(
    item for item in response.categories_analysis
    if item.category == TextCategory.VIOLENCE
)

# ðŸ–¨ï¸ Print results
if hate_result:
    print(f"ðŸ§¨ Hate severity: {hate_result.severity}")
if self_harm_result:
    print(f"ðŸ©¸ SelfHarm severity: {self_harm_result.severity}")
if sexual_result:
    print(f"ðŸ‘ Sexual severity: {sexual_result.severity}")
if violence_result:
    print(f"âš”ï¸ Violence severity: {violence_result.severity}")

# [END analyze_text]

if __name__ == "__main__":
    analyze_text()
```

---

## ðŸ” Output Example

Letâ€™s say we run this on our nasty sentence:

```plaintext
ðŸ§¨ Hate severity: 3
ðŸ©¸ SelfHarm severity: 0
ðŸ‘ Sexual severity: 0
âš”ï¸ Violence severity: 5
```

This means:

- Mild hate speech (probably the â€œidiotâ€)
- Severe violent threat (â€œI will kill youâ€)
- No adult content
- No suicidal indication

> You can use severity logic like:
>
> - **0â€“1**: Allow âœ…
> - **2â€“4**: Flag for review âš ï¸
> - **5â€“6**: Block or report ðŸš¨

---

## ðŸ“½ï¸ Line-by-Line Breakdown

| Line Range | Whatâ€™s Happening                           | Emoji |
| ---------- | ------------------------------------------ | ----- |
| 1â€“6        | SDK and model imports                      | ðŸ“¦    |
| 9â€“10       | Replace with your **own credentials**      | ðŸ”‘    |
| 13         | Initialize SDK client using your key       | ðŸ§     |
| 16         | Define text input to be analyzed           | ðŸ’¬    |
| 19â€“27      | Execute analysis and handle any exceptions | ðŸš¨    |
| 30â€“45      | Pull severity values for each category     | ðŸ“Š    |
| 48â€“53      | Print categorized severity scores          | ðŸ–¨ï¸    |

---

## ðŸ“ˆ What Happens Behind the Scenes?

```mermaid
sequenceDiagram
    participant Dev as Your App
    participant SDK as Azure SDK
    participant SafetyAPI as Azure Content Safety API
    participant AI as Classification Model

    Dev->>SDK: Create AnalyzeTextOptions
    SDK->>SafetyAPI: Send POST request with text
    SafetyAPI->>AI: Analyze text for categories
    AI-->>SafetyAPI: Return scores per category
    SafetyAPI-->>SDK: Structured JSON response
    SDK-->>Dev: You get scores back
```

---

## ðŸ“Ž Tips for Real-World Use

- ðŸ›‘ **Never block on keyword only** â€“ always use severity scores.
- ðŸ§  Train your moderation rules based on historical data + category relevance.
- ðŸ‘€ Log and review flagged texts periodically to improve moderation accuracy.
- ðŸ§ª Test with **real samples from your users**, not just crafted cases.
- ðŸ” Protect keys using Azure Key Vault or environment variables.

---

## ðŸ”„ Comparing with LLMs?

| Feature        | LLMs (e.g. GPT-4)         | Azure Content Safety     |
| -------------- | ------------------------- | ------------------------ |
| Scope          | General-purpose reasoning | Purpose-built moderation |
| Speed          | Slower                    | âš¡ Fast (specialized)    |
| Cost           | ðŸ’¸ Higher                 | ðŸ’° Lower                 |
| Consistency    | Varies                    | ðŸ”’ Predictable           |
| Text-Only?     | âœ…                        | âœ…                       |
| Explainability | High                      | Medium                   |

> LLMs are awesome ðŸ§ , but for **high-volume, low-latency moderation**, specialized tools like Content Safety are ðŸ”¥.

---

## âœ… Summary Table

| Feature             | Value                             |
| ------------------- | --------------------------------- |
| SDK Package         | `azure-ai-contentsafety`          |
| API Endpoint        | `/analyzeText`                    |
| Response Format     | Severity (0â€“6) per category       |
| Categories Analyzed | Hate, Self-Harm, Sexual, Violence |
| Integration Style   | SDK, REST, or Azure Logic Apps    |
| Cost                | Pay-per-call (with free tier!) ðŸ’µ |

---

## ðŸ“š Further Resources

- [ðŸ“„ Official Docs â€“ Text Analysis](https://learn.microsoft.com/en-us/azure/ai-services/content-safety/how-to/text-moderation)
- [ðŸ§  Responsible AI Practices](https://learn.microsoft.com/en-us/azure/responsible-ai/overview)
- [ðŸ§ª Try the REST API](https://learn.microsoft.com/en-us/rest/api/cognitiveservices/content-safety/analyze-text)
- [ðŸ§° SDK on PyPI](https://pypi.org/project/azure-ai-contentsafety/)
