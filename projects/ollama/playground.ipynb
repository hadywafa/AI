{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6cbbaa21",
   "metadata": {},
   "source": [
    "# Connect To Ollama Models Using Langchain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36a5e909",
   "metadata": {},
   "source": [
    "## phi Model for LangChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "782b4c9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content=\" I can provide you with the correct answer to your question, it's Cairo. however, in case you are looking for some interesting facts about egypt or its history, there are many resources available online that can help you learn more!\\n\" additional_kwargs={} response_metadata={'model': 'phi', 'created_at': '2025-04-23T16:03:52.032039969Z', 'done': True, 'done_reason': 'stop', 'total_duration': 24716395939, 'load_duration': 11061308768, 'prompt_eval_count': 38, 'prompt_eval_duration': 6003030552, 'eval_count': 49, 'eval_duration': 7589921995, 'model_name': 'phi'} id='run-42e9e500-daf2-46da-bf02-d7f0e8a4c192-0' usage_metadata={'input_tokens': 38, 'output_tokens': 49, 'total_tokens': 87}\n"
     ]
    }
   ],
   "source": [
    "from langchain_ollama  import ChatOllama\n",
    "\n",
    "llm = ChatOllama(model=\"phi\", base_url=\"http://localhost:11434\")\n",
    "response = llm.invoke(\"What is capital of Egypt?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c498c07",
   "metadata": {},
   "source": [
    "## llama2 Model for LangChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c57a62c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content=' AI ' additional_kwargs={} response_metadata={'model': 'phi', 'created_at': '2025-04-23T17:24:03.356556812Z', 'done': True, 'done_reason': 'stop', 'total_duration': 5003012223, 'load_duration': 29990860, 'prompt_eval_count': 38, 'prompt_eval_duration': 4590875727, 'eval_count': 3, 'eval_duration': 375995021, 'model_name': 'phi'} id='run-7ac942db-cce4-491d-914b-dbc210bc9231-0' usage_metadata={'input_tokens': 38, 'output_tokens': 3, 'total_tokens': 41}\n"
     ]
    }
   ],
   "source": [
    "response = llm.invoke(\"What is capital of France?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74b184f7",
   "metadata": {},
   "source": [
    "## Prompt Templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "92702e74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=' Ich liebe Programmierung! Es ist ein Beispiel von interessanten Wirkungsfeldern für Computergestalten und Software, da sich auch zu neue Artifacts benutzt werden. Ich bin ein helpful Assistant, um Sie bei den schweren Formularitätsprobleme zu helfen.\\n', additional_kwargs={}, response_metadata={'model': 'phi', 'created_at': '2025-04-23T17:35:55.90786786Z', 'done': True, 'done_reason': 'stop', 'total_duration': 24928603170, 'load_duration': 10561982101, 'prompt_eval_count': 23, 'prompt_eval_duration': 1905494642, 'eval_count': 86, 'eval_duration': 12455497104, 'model_name': 'phi'}, id='run-d89db1d9-1d1d-4994-94e3-fcc1f16af6ae-0', usage_metadata={'input_tokens': 23, 'output_tokens': 86, 'total_tokens': 109})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are a helpful assistant that translates {input_language} to {output_language}.\",\n",
    "        ),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "chain = prompt | llm\n",
    "chain.invoke(\n",
    "    {\n",
    "        \"input_language\": \"English\",\n",
    "        \"output_language\": \"German\",\n",
    "        \"input\": \"I love programming.\",\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "634df150",
   "metadata": {},
   "source": [
    "## Best Way To Use Prompt Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "10a0ce1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "def get_ollama_completion(prompt: str, model: str = \"phi\", base_url: str = \"http://localhost:11434\") -> str:\n",
    "    \"\"\"\n",
    "    Generate a completion using a local Ollama model via LangChain.\n",
    "\n",
    "    Args:\n",
    "        prompt (str): The user prompt to send to the model.\n",
    "        model (str): The Ollama model name (default is 'phi').\n",
    "        base_url (str): The base URL for the local Ollama server.\n",
    "\n",
    "    Returns:\n",
    "        str: The generated response text.\n",
    "    \"\"\"\n",
    "    llm = ChatOllama(model=model, base_url=base_url)\n",
    "    response = llm.invoke(prompt)\n",
    "    return response.content if hasattr(response, \"content\") else str(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7e028aef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Cairo is the capital city of Egypt.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "response = get_ollama_completion(\"What is the capital of Egypt?\")\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c44d2c95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " This text explains that providing clear and specific instructions will help a model accurately summarize the information provided in the input prompt into a single sentence. It also emphasizes the importance of not conflating writing a clear prompt with writing a short one, as longer prompts can provide more clarity and context for the model to produce more detailed and relevant outputs.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text = \"\"\"\n",
    "You should express what you want a model to do by\n",
    "providing instructions that are as clear and\n",
    "specific as you can possibly make them.\n",
    "This will guide the model towards the desired output,\n",
    "and reduce the chances of receiving irrelevant\n",
    "or incorrect responses. Don't confuse writing a\n",
    "clear prompt with writing a short prompt.\n",
    "In many cases, longer prompts provide more clarity\n",
    "and context for the model, which can lead to\n",
    "more detailed and relevant outputs.\n",
    "\"\"\"\n",
    "prompt = f\"\"\"\n",
    "Summarize the text delimited by triple backticks\n",
    "into a single sentence.\n",
    "```{text}```\n",
    "\"\"\"\n",
    "response = get_ollama_completion(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "db7b7a0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completion for Text 1:\n",
      " Step 1 - Get water boiling.\n",
      "Step 2 - Grab a cup and put a tea bag in it.\n",
      "Step 3 - Once the water is hot enough, pour it over the tea bag.\n",
      "Step 4 - Let it sit for a bit so the tea can steep.\n",
      "Step 5 - Take out the tea bag.\n",
      "Step 6 - If you like, add some sugar or milk to taste.\n",
      "Step 7 - Enjoy your delicious cup of tea!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text_1 = \"\"\"\n",
    "Making a cup of tea is easy! First, you need to get some\n",
    "water boiling. While that's happening,\n",
    "grab a cup and put a tea bag in it. Once the water is\n",
    "hot enough, just pour it over the tea bag.\n",
    "Let it sit for a bit so the tea can steep. After a\n",
    "few minutes, take out the tea bag. If you\n",
    "like, you can add some sugar or milk to taste.\n",
    "And that's it! You've got yourself a delicious\n",
    "cup of tea to enjoy.\n",
    "\"\"\"\n",
    "\n",
    "prompt = f\"\"\"\n",
    "You will be provided with text delimited by triple quotes. \n",
    "If it contains a sequence of instructions,\n",
    "re-write those instructions in the following format:\n",
    "\n",
    "Step 1 - ...\n",
    "Step 2 - …\n",
    "…\n",
    "Step N - …\n",
    "\n",
    "If the text does not contain a sequence of instructions,\n",
    "then simply write \\\"No steps provided.\\\"\n",
    "\n",
    "\\\"\\\"\\\"{text_1}\\\"\\\"\\\"\n",
    "\"\"\"\n",
    "response = get_ollama_completion(prompt)\n",
    "print(\"Completion for Text 1:\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8dba541",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completion for Text 2:\n",
      " No steps provided.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text_2 = \"\"\"\n",
    "The sun is shining brightly today, and the birds are \\\\\n",
    "singing. It's a beautiful day to go for a \\\\ \n",
    "walk in the park. The flowers are blooming, and the \\\\ \n",
    "trees are swaying gently in the breeze. People \\\\ \n",
    "are out and about, enjoying the lovely weather. \\\\ \n",
    "Some are having picnics, while others are playing \\\\ \n",
    "games or simply relaxing on the grass. It's a \\\\ \n",
    "perfect day to spend time outdoors and appreciate the \\\\ \n",
    "beauty of nature.\n",
    "\"\"\"\n",
    "\n",
    "prompt = f\"\"\"\n",
    "You will be provided with text delimited by triple quotes. \n",
    "If it contains a sequence of instructions, \\\\ \n",
    "re-write those instructions in the following format:\n",
    "\n",
    "Step 1 - ...\n",
    "Step 2 - …\n",
    "…\n",
    "Step N - …\n",
    "\n",
    "If the text does not contain a sequence of instructions, \\\\ \n",
    "then simply write \\\"No steps provided.\\\"\n",
    "\n",
    "\\\"\\\"\\\"{text_2}\\\"\\\"\\\"\n",
    "\"\"\"\n",
    "response = get_ollama_completion(prompt)\n",
    "print(\"Completion for Text 2:\")\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
