# ğŸ“¦ Plan and Implement a Container Deployment in Azure AI Services

Running AI models in containers is critical for **low-latency**, **high-security**, or **offline** scenarios. Azure AI Services allow you to deploy models as Docker containers locally or in your own infrastructure, while still tracking usage for billing.

This is especially useful for scenarios like:

- ğŸ” Military or defense systems (no internet allowed)
- âš¡ Real-time systems (e.g., missile tracking, CCTV analytics)
- ğŸ§  Custom GPU acceleration (using more powerful GPUs than Azure provides)

---

## ğŸ” #1 Why Use Azure AI Containers Instead of Cloud API?

### âœ… Use Azure AI Containers When:

| Scenario                                            | Why It Matters                                                           |
| --------------------------------------------------- | ------------------------------------------------------------------------ |
| ğŸ§  You need **data privacy**                        | Data stays inside your private network; no Internet needed for inference |
| âš¡ You need **low latency**                         | No round trip to Azure cloud = faster real-time performance              |
| ğŸ® You want **hardware control**                    | Use your own high-performance GPUs (e.g., RTX 4090, H100, Jetson)        |
| ğŸ›°ï¸ You're in an **offline** or isolated environment | Defense, medical, industrial edge, etc.                                  |

---

## ğŸ’° #2 How Does Azure Bill You for Local Container Usage?

Even though the **model runs locally**, **Azure still bills you** based on the number of API calls (inference requests).

### ğŸ“¦ Here's How:

- You must **provision a Language/Speech/Vision resource in Azure**
- Then pass its **Billing Endpoint + API Key** to the Docker container
- The container **calls home to Azure** _only for billing/accounting_, not for inference

```bash
docker run \
  Billing="https://<region>.api.cognitive.microsoft.com" \
  ApiKey="abc123..."
```

â›” **If the machine is fully offline**, you'd need a **special offline licensing agreement** with Microsoft.

---

## ğŸ¤” #3 Why Not Just Use Cloud API with Private Endpoint?

Azure also allows you to access AI Services over **Private Link**, which exposes a **private IP** inside your VNet.

### ğŸ” Container vs. Private Link:

| Feature                     | Azure AI Container                        | Azure AI with Private Link               |
| --------------------------- | ----------------------------------------- | ---------------------------------------- |
| ğŸ“ Location                 | Your server / edge device / on-prem       | Azure-hosted, accessed via private IP    |
| ğŸ“¶ Internet Required        | âŒ (only billing required)                | âŒ Fully internal using Azure networking |
| ğŸ’¾ Model Execution          | Locally inside Docker                     | Runs in Azure cloud                      |
| ğŸ® GPU Control              | âœ… Use your own GPUs                      | âŒ GPU choice is managed by Azure        |
| âš™ï¸ Infrastructure Ownership | Yours                                     | Azure-managed                            |
| ğŸ“¦ Use Case                 | Real-time, edge, defense, custom hardware | Secure enterprise integration            |

ğŸ§  Use Private Link when:

- You want **secure access to Azure AI**, but donâ€™t need custom GPUs or offline processing
- You're okay with Azure managing infrastructure, but want no public internet exposure

ğŸ§  Use Containers when:

- You need **100% local execution**, **custom performance**, or **compliance isolation**

---

## ğŸ§° #4 Requirements

- Docker installed on Linux (recommended)
- Azure Subscription and Speech resource created
- Billing endpoint + API key from Azure

---

## ğŸ› ï¸ #5 Step-by-Step: Container Deployment Walkthrough

### ğŸ§² Step 1: Pull the Official Azure Cognitive Services Container Image

```bash
docker pull mcr.microsoft.com/azure-cognitive-services/speechservices/neural-text-to-speech
```

This image includes the TTS model and all dependencies.

---

### ğŸ§ª Step 2: Run the Container

```bash
docker run --rm -it -d -p 5000:5000 \
  mcr.microsoft.com/azure-cognitive-services/speechservices/neural-text-to-speech \
  Eula=accept \
  Billing="https://<your-region>.api.cognitive.microsoft.com/" \
  ApiKey="<your-api-key>" \
  Logging:Console:LogLevel:Default=Information
```

- `-d` runs it in the background
- Port `5000` is exposed
- Billing info is still required even though execution is local

---

### âœ… Step 3: Confirm Container is Running

```bash
curl http://localhost:5000/          # Returns welcome message
curl http://localhost:5000/status    # Returns running status
curl http://localhost:5000/ready     # Confirms API key and readiness
```

---

### ğŸ§  Step 4: Send a Local API Request (Text-to-Speech)

```bash
curl -v -X POST "http://localhost:5000/cognitiveservices/v1" \
     -H "Content-Type: application/ssml+xml" \
     -H "X-MICROSOFT-OutputFormat: riff-48khz-16bit-mono-pcm" \
     --data @speech.xml --output output.wav
```

- Use `SSML` (Speech Synthesis Markup Language) in `speech.xml`
- Output is saved to `output.wav`

---

### ğŸ§ª Bonus: Call Container with Python SDK

```python
import azure.cognitiveservices.speech as speechsdk

speech_config = speechsdk.SpeechConfig(host="http://localhost:5000")
audio_config = speechsdk.audio.AudioOutputConfig(filename="bbg.wav")
speech_synthesizer = speechsdk.SpeechSynthesizer(speech_config=speech_config, audio_config=audio_config)

ssml = open("/home/jo/speech.xml").read()
speech_synthesizer.speak_ssml_async(ssml).get()
```

Use your local endpoint in the SDK instead of Azure.

---

## ğŸ“¦ #6 Common Use Cases

### ğŸ›°ï¸ Real-Time Object Detection

Run Azure Spatial Analysis containers for instant webcam-based analytics with no cloud latency.

### ğŸ§¬ Medical Devices

Run Form Recognizer locally on radiology reports inside hospital networks.

### ğŸ§  LLMs + Edge Devices

Deploy language understanding locally using high-end GPUs like RTX 4090.

---

## ğŸ’¡ #7 Notes & Tips

- Containers require billing endpoints even when run offline.
- You can scale them manually or orchestrate with Kubernetes.
- Use ACR (Azure Container Registry) for your own image hosting.
- Performance is hardware-dependent: use GPU VMs or edge boxes for best results.

---

## ğŸ”’ #8 Security Considerations

| Concern             | Mitigation                           |
| ------------------- | ------------------------------------ |
| Data Privacy        | Keep all inference in local network  |
| Unauthorized Access | Bind container to internal IP only   |
| Network Egress      | Block internet if billing is prepaid |

---

## ğŸ“š #9 References

- [Text to Speech Container Docs](https://learn.microsoft.com/en-us/azure/ai-services/speech-service/speech-container-howto)
- [REST TTS API](https://learn.microsoft.com/en-us/azure/ai-services/speech-service/rest-text-to-speech?tabs=streaming)
- [Azure Container Instances](https://learn.microsoft.com/en-us/azure/container-instances/)
- [Azure Container Registry (ACR)](https://learn.microsoft.com/en-us/azure/container-registry/)

## ğŸ“’ Notes

### â‰ï¸ Question 1

<div style="text-align: center;">
    <img src="images/q1.png" alt="q1" style="border-radius: 10px; width: 60%;">
</div>

---

**Scenario**:

- You want to do **Sentiment Analysis**
- Data must stay **on-premises**
- But you still want to use Azure's pretrained Language model

**âœ… Option A is correct:**

- **Provision the Language service in Azure** > **Deploy Docker container to an on-premises server** > **Run container and query the endpoint**

**âŒ Why Others Are Wrong:**

- **B**: It skips provisioning the Language service (needed for billing)
- **C**: Deploys to Azure â€” but the **requirement** is to keep data internal!
- **D**: Incorrect â€” Option A satisfies everything

---

### â‰ï¸ Question 2

<div style="text-align: center;">
    <img src="images/q2.png" alt="q2" style="border-radius: 10px; width: 60%;">
</div>

---

The correct answer is âœ… **C. All the statements are true** â€” and hereâ€™s **why**, with a breakdown of each statement:

Each of the three endpoints plays a different role:

| Endpoint   | Purpose                                       |
| ---------- | --------------------------------------------- |
| `/ready`   | Confirm container is live and model is loaded |
| `/status`  | Verify billing API key with Azure             |
| `/swagger` | Developer UI and API reference                |

These are all part of Azureâ€™s [official container guidance](https://learn.microsoft.com/en-us/azure/ai-services/speech-service/speech-container-howto).
