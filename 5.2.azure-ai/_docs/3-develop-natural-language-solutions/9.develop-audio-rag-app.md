# üéÅ Develop an audio-enabled generative AI application

In this module, we'll discuss audio-enabled generative AI and explore how you can use Azure AI Foundry to create generative AI solutions that respond to prompts that include a mix of text and audio data.

To handle prompts that include audio, you need to deploy a multimodal generative AI model - in other words, a model that supports not only text-based input, but audio-based input as well. Multimodal models available in Azure AI Foundry include (among others):

- Microsoft Phi-4-multimodal-instruct
- OpenAI gpt-4o
- OpenAI gpt-4o-mini

The JSON representation of a prompt that includes a multi-part user message looks something like this:

```json
{
  "messages": [
    { "role": "system", "content": "You are a helpful assistant." },
    {
      "role": "user",
      "content": [
        {
          "type": "text",
          "text": "Transcribe this audio:"
        },
        {
          "type": "audio_url",
          "audio_url": {
            "url": "https://....."
          }
        }
      ]
    }
  ]
}
```

The audio content item can be:

- A URL to an audio file in a web site.
- Binary audio data

```json
{
  "type": "audio_url",
  "audio_url": {
    "url": "data:audio/mp3;base64,<binary_audio_data>"
  }
}
```
