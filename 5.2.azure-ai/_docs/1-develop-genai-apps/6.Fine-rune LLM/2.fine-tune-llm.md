# Fine-tune LLM

## Prepare your data to fine-tune a chat completion model

Fine-tuning involves combining a suitable foundation model to use as a base, and with a set of training data that includes example prompts and responses that the model can learn from.

![fine-tuning-llm](images/fine-tuning-llm.png)

---

When you decide you want to fine-tune a language model, you need to identify the dataset you can use to fine-tune your language model.

Similar to any machine learning model, the quality of the dataset has a large effect on the quality of your model. Though you need less data than when you would train a language model from scratch, you still might need enough data to maximize the consistency of your desired model's behavior. How much data you need depends on your use case.

### Example Dataset

When you fine-tune a language model for chat completion, the data you use to fine-tune a model is a collection of sample conversations. More specifically, the data should contain three components:

1. The system message
1. The user message
1. The assistant's response

```json
{
  "messages": [
    {
      "role": "system",
      "content": "You are an Xbox customer support agent whose primary goal is to help users with issues they are experiencing with their Xbox devices. You are friendly and concise. You only provide factual answers to queries, and do not provide answers that are not related to Xbox."
    },
    { "role": "user", "content": "Is Xbox better than PlayStation?" },
    {
      "role": "assistant",
      "content": "I apologize, but I cannot provide personal opinions. My primary job is to assist you with any issues related to your Xbox device. Do you have any Xbox-related issues that need addressing?"
    }
  ]
}
```

### An example of a multi-turn chat file format with weights:

You can include multiple turns of a conversation on a single line in the dataset. If you want to fine-tune only on specific assistant messages, you can optionally use the weight key-value pair. When the weight is set to 0, the message is ignored, when you set to 1, the message is included for training.

```json
{
  "messages": [
    { "role": "system", "content": "Marv is a factual chatbot that is also sarcastic." },
    { "role": "user", "content": "What's the capital of France?" },
    { "role": "assistant", "content": "Paris", "weight": 0 },
    { "role": "user", "content": "Can you be more sarcastic?" },
    { "role": "assistant", "content": "Paris, as if everyone doesn't know that already.", "weight": 1 }
  ]
}
```

## Explore fine-tuning language models in Azure AI Studio

When you want to fine-tune a language model, you can use a base or foundation model that is already pretrained on large amounts of data. There are many foundation models available through the model catalog in the Azure AI Studio. You can fine-tune base models on various tasks, like text classification, translation, or chat completion.

When you want to use a fine-tuned model to generate responses in a chat application, you need to use a base model that can be fine-tuned on a chat completion task. The Azure AI Studio model catalog allows you to filter based on fine-tuning tasks to decide which base model to select. You can, for example, select a GPT-4 or Llama-2-7b model to fine-tune on your own training data.
