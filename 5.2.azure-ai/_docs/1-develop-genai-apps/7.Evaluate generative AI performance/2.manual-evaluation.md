# âœ… Manually Evaluate Your Model in Azure AI Foundry

---

## ğŸ¤” Why Do Manual Evaluation?

Even with all the automation, **human feedback is still key**!
Manual evaluation helps you check:

- Is the model answering clearly?
- Is it giving helpful or confusing answers?
- Is it handling edge cases properly?

This is useful:

- âœ… Early in development (when you're testing)
- âœ… Even after deploying (for quality checks)

---

## ğŸ§ª Step 1: Prepare Your Test Prompts

Before testing anything, you need a good **set of example questions**.

Make sure your test prompts include:

- ğŸŸ¢ Common user questions (ex: "What is Azure?")
- ğŸŸ¡ Edge cases (ex: "What if I lose my key?")
- ğŸ”´ Trick/failure cases (ex: vague, unclear prompts)

ğŸ“Œ These help test the model from all angles.

---

## ğŸ’¬ Step 2: Use the Chat Playground

Want to test one prompt quickly? Use the **Chat Playground** in Azure AI Foundry.

Hereâ€™s what you can do:

- Type a question and see the model's answer.
- Adjust your prompt or system message if needed.
- Try again and see if it improves!

ğŸ› ï¸ This is great for **prompt tuning** and small experiments.

---

![ChatPlayground](images/ChatPlayground.png)

---

## ğŸ“„ Step 3: Evaluate Multiple Prompts with Manual Evaluation

When youâ€™re ready to test **many prompts at once**:

1. Upload a dataset (like a table or CSV) with:

   - Test questions
   - (Optional) Expected answers

2. Azure runs all those prompts through the model.

3. You can review and **rate each response** (ğŸ‘ / ğŸ‘).

ğŸ¯ This helps you spot patterns â€” like what the model gets wrong often.

---

![manual-evaluation-for-multiple-prompts](images/manual-evaluation-for-multiple-prompts.png)

---

## ğŸ§  What Can You Improve Based on Results?

If the model fails or underperforms, try adjusting:

| What to Change      | Why                                   |
| ------------------- | ------------------------------------- |
| ğŸ”§ The prompt       | Maybe your wording is unclear         |
| ğŸ§¾ System message   | Set better instructions for the model |
| ğŸ¤– Model type       | Try GPT-4 instead of GPT-3.5          |
| ğŸšï¸ Model parameters | Adjust temperature, max tokens, etc.  |

---

## ğŸ” Final Step: Evaluate Prompt Flows Too

Once youâ€™ve integrated the model into a **prompt flow** (the full logic of your app), you can **manually evaluate the flow** as well â€” not just the model.

This helps make sure your **whole app** is behaving correctly, not just one model.

---

## ğŸ“ Summary

| Step                    | What You Do                        |
| ----------------------- | ---------------------------------- |
| 1ï¸âƒ£ Prepare test prompts | Include various types of questions |
| 2ï¸âƒ£ Test single prompt   | Use the Chat Playground            |
| 3ï¸âƒ£ Test many prompts    | Use the Manual Evaluation feature  |
| 4ï¸âƒ£ Improve model        | Based on ratings, tweak your setup |
| 5ï¸âƒ£ Evaluate flows       | Test full app behavior too         |
