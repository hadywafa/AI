# ğŸ’¬ Azure AI Content Safety â€“ Text Analysis with Python

## ğŸ§¾ Official Overview

> **Azure AI Content Safety** helps detect and classify harmful user-generated text across **4 categories**, returning severity scores (0 to 6). Itâ€™s built for **real-time moderation**, **policy enforcement**, and **keeping your platform safe**.

### Categories it detects

| Category     | Description                                 |
| ------------ | ------------------------------------------- |
| ğŸ§¨ Hate      | Racist, discriminatory, or abusive language |
| ğŸ©¸ Self-Harm | Mentions of suicide, self-injury            |
| ğŸ‘ Sexual    | Inappropriate, adult, or suggestive content |
| âš”ï¸ Violence  | Threats, gore, violent expressions          |

---

## ğŸ§° Prerequisites

1. ğŸ§  Azure Subscription
2. â˜ï¸ Provision **Azure AI Content Safety** resource
3. ğŸ”‘ Get `endpoint` and `key` from Azure Portal
4. ğŸ› ï¸ Install SDK

   ```bash
   pip install azure-ai-contentsafety
   ```

---

## ğŸ§ª Full Code Example: Analyzing Text for Bad Vibes

Hereâ€™s a full working example using the official `azure-ai-contentsafety` SDK â€” fully explained below.

### ğŸ§¾ Python Script

```python
# Azure AI ContentSafety Text Analysis Example
# pip install azure-ai-contentsafety

import os
from azure.ai.contentsafety import ContentSafetyClient
from azure.ai.contentsafety.models import TextCategory, AnalyzeTextOptions
from azure.core.credentials import AzureKeyCredential
from azure.core.exceptions import HttpResponseError

# ğŸ‘® Replace with your real keys
key = "f7324ffadc7e4da0b5b253230a44a849"
endpoint = "https://azure-content-safety-demo-345.cognitiveservices.azure.com/"

# ğŸ”§ Initialize the Content Safety client
client = ContentSafetyClient(endpoint, AzureKeyCredential(key))

# ğŸ—¯ï¸ Suspicious text to analyze
request = AnalyzeTextOptions(text="You are an idiot. I will kill you.")

# ğŸ§  Call the API to analyze text
try:
    response = client.analyze_text(request)
except HttpResponseError as e:
    print("Analyze text failed.")
    if e.error:
        print(f"Error code: {e.error.code}")
        print(f"Error message: {e.error.message}")
    raise

# ğŸ“Š Extract and display results
hate_result = next(
    item for item in response.categories_analysis
    if item.category == TextCategory.HATE
)
self_harm_result = next(
    item for item in response.categories_analysis
    if item.category == TextCategory.SELF_HARM
)
sexual_result = next(
    item for item in response.categories_analysis
    if item.category == TextCategory.SEXUAL
)
violence_result = next(
    item for item in response.categories_analysis
    if item.category == TextCategory.VIOLENCE
)

# ğŸ–¨ï¸ Print results
if hate_result:
    print(f"ğŸ§¨ Hate severity: {hate_result.severity}")
if self_harm_result:
    print(f"ğŸ©¸ SelfHarm severity: {self_harm_result.severity}")
if sexual_result:
    print(f"ğŸ‘ Sexual severity: {sexual_result.severity}")
if violence_result:
    print(f"âš”ï¸ Violence severity: {violence_result.severity}")

# [END analyze_text]

if __name__ == "__main__":
    analyze_text()
```

---

## ğŸ” Output Example

Letâ€™s say we run this on our nasty sentence:

```plaintext
ğŸ§¨ Hate severity: 3
ğŸ©¸ SelfHarm severity: 0
ğŸ‘ Sexual severity: 0
âš”ï¸ Violence severity: 5
```

This means:

- Mild hate speech (probably the â€œidiotâ€)
- Severe violent threat (â€œI will kill youâ€)
- No adult content
- No suicidal indication

> You can use severity logic like:
>
> - **0â€“1**: Allow âœ…
> - **2â€“4**: Flag for review âš ï¸
> - **5â€“6**: Block or report ğŸš¨

---

## ğŸ“½ï¸ Line-by-Line Breakdown

| Line Range | Whatâ€™s Happening                           | Emoji |
| ---------- | ------------------------------------------ | ----- |
| 1â€“6        | SDK and model imports                      | ğŸ“¦    |
| 9â€“10       | Replace with your **own credentials**      | ğŸ”‘    |
| 13         | Initialize SDK client using your key       | ğŸ§     |
| 16         | Define text input to be analyzed           | ğŸ’¬    |
| 19â€“27      | Execute analysis and handle any exceptions | ğŸš¨    |
| 30â€“45      | Pull severity values for each category     | ğŸ“Š    |
| 48â€“53      | Print categorized severity scores          | ğŸ–¨ï¸    |

---

## ğŸ“ˆ What Happens Behind the Scenes?

<div align="center">

```mermaid
sequenceDiagram
    participant Dev as Your App
    participant SDK as Azure SDK
    participant SafetyAPI as Azure Content Safety API
    participant AI as Classification Model

    Dev->>SDK: Create AnalyzeTextOptions
    SDK->>SafetyAPI: Send POST request with text
    SafetyAPI->>AI: Analyze text for categories
    AI-->>SafetyAPI: Return scores per category
    SafetyAPI-->>SDK: Structured JSON response
    SDK-->>Dev: You get scores back
```

</div>

---

## ğŸ“ Tips for Real-World Use

- ğŸ›‘ **Never block on keyword only** â€“ always use severity scores.
- ğŸ§  Train your moderation rules based on historical data + category relevance.
- ğŸ‘€ Log and review flagged texts periodically to improve moderation accuracy.
- ğŸ§ª Test with **real samples from your users**, not just crafted cases.
- ğŸ” Protect keys using Azure Key Vault or environment variables.

---

## ğŸ”„ Comparing with LLMs?

| Feature        | LLMs (e.g. GPT-4)         | Azure Content Safety     |
| -------------- | ------------------------- | ------------------------ |
| Scope          | General-purpose reasoning | Purpose-built moderation |
| Speed          | Slower                    | âš¡ Fast (specialized)    |
| Cost           | ğŸ’¸ Higher                 | ğŸ’° Lower                 |
| Consistency    | Varies                    | ğŸ”’ Predictable           |
| Text-Only?     | âœ…                        | âœ…                       |
| Explainability | High                      | Medium                   |

> LLMs are awesome ğŸ§ , but for **high-volume, low-latency moderation**, specialized tools like Content Safety are ğŸ”¥.

---

## âœ… Summary Table

| Feature             | Value                             |
| ------------------- | --------------------------------- |
| SDK Package         | `azure-ai-contentsafety`          |
| API Endpoint        | `/analyzeText`                    |
| Response Format     | Severity (0â€“6) per category       |
| Categories Analyzed | Hate, Self-Harm, Sexual, Violence |
| Integration Style   | SDK, REST, or Azure Logic Apps    |
| Cost                | Pay-per-call (with free tier!) ğŸ’µ |

---

## ğŸ“š Further Resources

- [ğŸ“„ Official Docs â€“ Text Analysis](https://learn.microsoft.com/en-us/azure/ai-services/content-safety/how-to/text-moderation)
- [ğŸ§  Responsible AI Practices](https://learn.microsoft.com/en-us/azure/responsible-ai/overview)
- [ğŸ§ª Try the REST API](https://learn.microsoft.com/en-us/rest/api/cognitiveservices/content-safety/analyze-text)
- [ğŸ§° SDK on PyPI](https://pypi.org/project/azure-ai-contentsafety/)
