# ğŸ¤–ğŸ§© **Prompt Flow in Azure AI Foundry** â€“ From Basics to Advanced

> The beating heart of modern Azure-based LLM applications â€” visually programmable, prompt-enhanced, and Python-powered.

---

<div style="text-align: center;">
    <img src="images/prompt_flow.png" alt="prompt_flow" style="border-radius: 10px; width: 60%;">
</div>

---

## ğŸš€ What Is Prompt Flow?

> **Prompt Flow** is an orchestration system inside **Azure AI Foundry** (and Azure Machine Learning) that lets you build, test, debug, deploy, and monitor **LLM workflows** using **modular nodes** â€” combining prompts, Python code, and AI services.

Think of it like:

- ğŸ§  Prompt Engineering â•
- ğŸ› ï¸ Code Tooling â•
- ğŸ” Workflow Engine â•
- ğŸ“¦ Deployment pipeline â•
- ğŸ“Š Evaluation framework

All wrapped inside a GUI **flow graph** + YAML + code.

---

## ğŸ¯ Why Prompt Flow Exists?

LLMs (like GPT-4) donâ€™t work in isolation. Real-world LLM apps need:

- Preprocessing (PDF â†’ text, HTML cleaning, etc.)
- Prompt tuning and template filling
- Calling external tools/APIs to augment answers
- Postprocessing and validation
- Evaluation and debugging

âš¡ **Prompt Flow** solves these **LLM app complexity problems** â€” visually and programmatically.

---

## ğŸ“¦ Prompt Flow Components (Core Building Blocks)

### ğŸ§© 1. **Nodes**

A Prompt Flow is a **DAG (Directed Acyclic Graph)** made up of connected **nodes**:

| Node Type | Purpose                                      |
| --------- | -------------------------------------------- |
| `llm`     | Call OpenAI model (e.g. GPT-4o-mini)         |
| `prompt`  | Jinja2 template with structured instructions |
| `python`  | Run scripts or preprocessing tools           |
| `data`    | Load structured input like CSV               |
| `file`    | Read static text / sample docs               |

Each node takes inputs, processes something, and passes output to the next.

---

### ğŸ“„ 2. **Prompt Tool**

- Jinja2-powered template with support for:

  - Chat history rendering
  - Input variables (`{{ question }}`)
  - Few-shot examples
  - System-level instructions

ğŸ§  It's where **prompt engineering** happens.

---

### ğŸ§  3. **LLM Tool**

- Calls Azure OpenAI (e.g., `gpt-35-turbo`, `gpt-4`, or `gpt-4o`)
- Supports:

  - Temperature, max_tokens, top_p
  - System vs user role
  - Format: `chat`, `json`, `text`

---

### ğŸ 4. **Python Tool**

- Run real Python code in the pipeline:

  - Call APIs (e.g., search Wikipedia, query SQL)
  - Process PDFs, images, HTML
  - Add grounding logic (e.g. search, vector DBs)
  - Postprocess outputs (cleaning, summarizing)

ğŸ“Œ Uses `@tool` decorators to define functions.

---

### ğŸ’¾ 5. **Compute Sessions**

To run Python and LLM tools, Prompt Flow uses **Compute Sessions**:

- Backed by Azure ML Compute (Docker/VM)
- Cost is incurred during execution
- Required for flow **debugging**, **chatting**, and **evaluation**

---

## ğŸ§± Flow DAG (Architecture)

Each Prompt Flow is defined by:

- `flow.dag.yaml`: Graph definition with input/output mapping
- `*.jinja`: Prompt templates
- `*.py`: Custom tool scripts
- `inputs.json`: Optional test data

ğŸ“Œ Think of it as a **self-contained LLM app**.

---

## ğŸ”„ Prompt Flow Types

Prompt Flows are **categorized by purpose**:

| Type                   | Description                                             |
| ---------------------- | ------------------------------------------------------- |
| **Standard**           | General-purpose flow with mixed tools                   |
| **Chat**               | Conversational multi-turn format with chat history      |
| **Evaluation**         | For evaluating flows across sample datasets and metrics |
| **Streaming (coming)** | Designed for real-time streaming of inputs              |

You choose the type when creating or cloning a flow.

---

## ğŸ“Š Use Cases & Examples

| Use Case                      | How Prompt Flow Helps                        |
| ----------------------------- | -------------------------------------------- |
| Chatbot with grounding        | Search Wikipedia, inject text into prompt    |
| RAG (Retrieval-Augmented Gen) | Fetch from vector DB, inject as context      |
| PDF Q\&A                      | Convert PDF to text, extract Q\&A, summarize |
| Multi-modal                   | Process images, audio before prompting       |
| Data-to-text summary          | Python â†’ JSON â†’ LLM-generated text           |
| Evaluation                    | A/B test multiple prompt versions            |

---

## ğŸ› ï¸ Real Example: â€œChat with Wikipediaâ€

Flow steps:

1. Input: â€œWhat is the capital of India?â€
2. `llm`: Extract search keywords â†’ â€œcapital of Indiaâ€
3. `python`: Search Wikipedia using `requests` + `BeautifulSoup`
4. `python`: Parse HTML text and clean it
5. `prompt` + `llm`: Ask the final question using Wikipedia text as **context**
6. Output: â€œNew Delhiâ€ (grounded response)

> ğŸ“Œ This is **prompt engineering + grounding = RAG** â€” implemented using Prompt Flow.

---

## ğŸ” Prompt Flow vs. Other Techniques

| Feature              | Prompt Engineering | RAG          | Prompt Flow         |
| -------------------- | ------------------ | ------------ | ------------------- |
| Manual Prompting     | âœ… Yes             | âš ï¸ Optional  | âœ… Yes (Jinja2)     |
| Uses external data   | âŒ No              | âœ… Yes       | âœ… Yes              |
| Handles Python logic | âŒ No              | âš ï¸ Partially | âœ… Yes              |
| Visual pipeline      | âŒ No              | âŒ No        | âœ… Yes (DAG)        |
| Evaluation tools     | âŒ No              | âš ï¸ Limited   | âœ… Yes (Built-in)   |
| Deployment           | âŒ Manual          | âŒ Manual    | âœ… One-click deploy |
| Monitoring           | âŒ No              | âš ï¸ Complex   | âœ… Built-in logs    |

---

## ğŸ“¥ How to Create a Prompt Flow

1. Go to **Azure AI Studio â†’ AI Foundry â†’ Prompt Flows**
2. Click â• `New Flow` or clone sample
3. Choose flow type (Standard, Chat, Eval)
4. Drag nodes: Input â†’ Prompt â†’ LLM â†’ Python
5. Define prompt template in `.jinja`
6. Attach compute
7. Run/debug the flow
8. Evaluate using datasets (optional)
9. Deploy to endpoint

---

## ğŸ”’ Permissions & Security

| Resource           | Required Role                                 |
| ------------------ | --------------------------------------------- |
| Azure AI Project   | `Contributor` or `ML Workspace User`          |
| Compute Instance   | `Reader` / `Contributor` on ML workspace      |
| Prompt Flow Access | Role inside **Hub-based project** (mandatory) |
| Blob/File Storage  | `Reader` or `Storage Blob Data Reader`        |

âœ… Use **Managed Identity** or `AzureMLCompute` role for automation.

---

## âš¡ Gotchas & Best Practices

- Always **start Compute** to run/debug flows
- Make sure to **stop sessions** to avoid extra billing
- Use `flow.dag.yaml` to version and manage flows as code
- Split logic: Prompt for prompting, Python for logic
- Add logging inside Python tools for debugging
- Evaluate flows regularly to detect prompt regressions
- Avoid massive prompt context â€” can hit token limits

---

## ğŸ”— Integration & Deployment

- Expose as REST API via Azure deployment
- Use in **Agents**, **Copilot**, or external apps
- Integrated with:

  - Azure AI Studio
  - Azure DevOps (CI/CD)
  - Azure ML Monitor
  - PromptFlow SDK / CLI / VS Code

---

## ğŸ“š Learn More

- [ğŸ”¹ Prompt Flow Concepts â€“ Microsoft Docs](https://learn.microsoft.com/en-us/azure/ai-foundry/concepts/prompt-flow)
- [ğŸ”¹ Prompt Flow GitHub & SDK](https://microsoft.github.io/promptflow/)
- [ğŸ”¹ YouTube Tutorial - Getting Started with Prompt Flow](https://www.youtube.com/watch?v=Q0udLVx07HY)

---

## ğŸ§  Summary

| Feature              | Value                              |
| -------------------- | ---------------------------------- |
| Visual LLM builder   | Yes, with DAG + Graph UI           |
| Python Integration   | Yes (custom tools with decorators) |
| Prompt Engineering   | Yes (Jinja2 templates)             |
| Evaluation           | Yes, using datasets and metrics    |
| Grounding support    | Yes (like RAG with custom logic)   |
| End-to-End App Ready | Yes â€“ deploy, monitor, iterate     |

Prompt Flow lets you build real-world, production-grade LLM apps â€” grounded, debuggable, and deployable.
