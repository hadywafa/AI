# ğŸ“ğŸ“Š Object Detection Evaluation Metrics

Youâ€™ve trained your model, and it starts drawing boxes around things. But how do you know itâ€™s doing a _good_ job?

That's where **Evaluation Metrics** come in!

---

## ğŸ”¢ Key Terms You Must Know

| Term                              | Description                                                       |
| --------------------------------- | ----------------------------------------------------------------- |
| **True Positive (TP)**            | Correctly detected object (right class, right location)           |
| **False Positive (FP)**           | Detected something that _wasnâ€™t there_ or misclassified an object |
| **False Negative (FN)**           | Missed a real object â€“ shouldâ€™ve detected it, but didnâ€™t          |
| **IoU (Intersection over Union)** | A measure of how much the predicted box overlaps the actual box   |

---

## ğŸ§® Object detection Metrics â€“ With Pizza ğŸ•

Letâ€™s say your job is to detect **pepperonis** on a pizza ğŸ• using object detection.

### ğŸ¯ **1.Precision**

> Out of all pepperonis you _found_, how many were **actually** pepperoni?

```text
Precision = TP / (TP + FP)
```

ğŸ¯ High precision = few false alarms

ğŸ” Use Case: Self-driving car doesnâ€™t want to think a mailbox is a child ğŸ˜¬

---

### ğŸ” **2.Recall**

> Out of all the real pepperonis on the pizza, how many did you actually find?

```text
Recall = TP / (TP + FN)
```

ğŸ¯ High recall = fewer misses

ğŸ” Use Case: Detecting tumors in medical scans â€“ you want to catch _everything_

---

### ğŸ“Š **3.mAP** â€“ Mean Average Precision

> **mAP** stands for **Mean Average Precision**. It is the **average** of the **precision scores** calculated at various **recall levels**, across all object categories.

Letâ€™s unpack it:

#### ğŸ§® Step-by-step

1. Calculate **Precision & Recall** at different confidence thresholds
2. Plot a **Precision-Recall Curve**
3. Compute the **Area Under Curve (AUC)** â†’ This is called **AP (Average Precision)** for one class
4. Repeat for each class â†’ dog, cat, pepperoni, etc.
5. Take the **mean** of all APs â†’ this is your **mAP**

---

#### ğŸ“ˆ Visual: Precision-Recall Curve

<div align="center">

```mermaid
graph LR
    A[Recall 0.0] --> B[Recall 0.2]
    B --> C[Recall 0.4]
    C --> D[Recall 0.6]
    D --> E[Recall 0.8]
    E --> F[Recall 1.0]

    A -->|Precision 1.0| B
    B -->|Precision 0.9| C
    C -->|Precision 0.8| D
    D -->|Precision 0.7| E
    E -->|Precision 0.6| F
```

</div>

> ğŸ“Œ The area under this curve = **AP** for one class.  
> ğŸ“Œ Average this across classes = **mAP**.  
> ğŸš¨ **mAP** This is the **most important** metric in object detection. AI-102 exam expects you to know this one well.

---

### ğŸŒŒ **4.IoU** â€“ Intersection over Union

It's how much your predicted box overlaps with the actual pepperoni.

<div align="center">

```mermaid
graph TD
    A[Predicted Box]
    B[Ground Truth Box]
    A -->|Overlap Area| C[Intersection]
    B -->|Total Area| D[Union]
```

</div>

ğŸ“ Formula:

```text
IoU = Area of Overlap / Area of Union
```

ğŸ” Typical thresholds: `0.5` or `0.75` (higher = stricter)

---

## ğŸ“¦ Azure Custom Vision mAP in Practice

When you **train your object detection model**, Azure shows:

- **mAP (Mean Average Precision)**
- **Precision**
- **Recall**
- **Per-tag metrics** too

| Tag     | Precision | Recall | AP (Area) |
| ------- | --------- | ------ | --------- |
| Car     | 0.93      | 0.88   | 0.90      |
| Bus     | 0.88      | 0.94   | 0.91      |
| Person  | 0.80      | 0.85   | 0.82      |
| **mAP** |           |        | **0.88**  |

---

## ğŸ§ª Sample Confusion Example

Letâ€™s say your object detection model:

- Correctly detects 4 cats (TP = 4)
- Misses 1 real cat (FN = 1)
- Incorrectly detects 2 dogs as cats (FP = 2)

ğŸ” Calculate:

```text
Precision = 4 / (4 + 2) = 0.67
Recall    = 4 / (4 + 1) = 0.80
```

---

## ğŸ§  Which Metric to Use When?

| Goal                                    | Use This Metric |
| --------------------------------------- | --------------- |
| Catch as many harmful items as possible | Recall          |
| Avoid false positives (high confidence) | Precision       |
| Balance both (overall performance)      | mAP             |
| Evaluate by class                       | Per-class AP    |

---

## ğŸ¢ Precision and Recall Tradeoff

You canâ€™t always have both. Imagine:

- High Precision, Low Recall = You detect only _perfect_ pepperonis, but miss some real ones.
- High Recall, Low Precision = You detect _everything that looks like pepperoni_, including olives ğŸ«’

---

## ğŸ§  Exam Tips â€“ Memorize This

- âœ… **IoU** â€“ How much your predicted box overlaps real object
- âœ… **Precision** â€“ Accuracy of your detections
- âœ… **Recall** â€“ How many objects you found
- âœ… **mAP** â€“ Mean of all average precision across classes
- âœ… Azure Custom Vision shows these right after training

---

## ğŸ§ª Want to Test This?

Head to the [Custom Vision Portal](https://customvision.ai), create an object detection project, upload images, draw boxes, and click **Train**.
Then go to the **Performance tab**, and youâ€™ll see:

```text
Mean Average Precision: 0.89
Precision: 0.91
Recall: 0.87
```
