# ğŸ§ª Image Classification Evaluation Metrics

Whether you're classifying **cats vs dogs**, or **pineapple vs pizza ğŸ•**, you need to know _how well_ your classifier performs.

> These metrics tell you **how often your AI is right**, how often it messes up, and **what kind of mistakes it makes**.

Letâ€™s go from **the basics â¡ï¸ all the way to advanced mAP and multi-class logic**, without fluff.

---

## ğŸ§  What Are We Measuring?

Your model predicts:

- â€œThis is a catâ€
- â€œThis is a hotdogâ€
- â€œThis is _not_ a hotdogâ€

Butâ€¦ was it _correct_? Letâ€™s see how we score it ğŸ”¢

---

## âš–ï¸ Key Metrics (And What They Mean)

| Metric               | Definition                                                               |
| -------------------- | ------------------------------------------------------------------------ |
| **1. `Precision`**   | Of the items `predicted` as â€œclass Xâ€, how many were truly class X?      |
| **2. `Recall`**      | Of all items that `actually` are â€œclass Xâ€, how many did the model find? |
| **3. `F1 Score`**    | The harmonic mean of Precision and Recall (balance metric)               |
| **Confusion Matrix** | Table that shows **what got confused with what** (super useful)          |
| **Accuracy**         | % of total predictions that were correct (basic score)                   |
| **mAP (optional)**   | If you enable probability thresholds, you can still compute mAP          |

---

## ğŸ© Sweet Analogy: Classifying Donuts

Youâ€™re building an AI to detect ğŸ© donuts vs ğŸ¥¯ bagels vs ğŸ¥ croissants.

Your AI says:

- ğŸ© â†’ Correct? âœ…
- ğŸ¥ â†’ Thought it was a bagel âŒ
- ğŸ¥¯ â†’ Missed it entirely âŒ

How do we score it?

---

### âœğŸ» Example

Letâ€™s say youâ€™re focused on just the ğŸ¥ class:

- **TP (True Positive):** ğŸ¥ correctly predicted
- **FP (False Positive):** Not ğŸ¥, but predicted as ğŸ¥
- **FN (False Negative):** Actually ğŸ¥, but missed it

### ğŸ¯ 1.Precision

```text
Precision = TP / (TP + FP)
```

â¡ï¸ _Out of all predicted ğŸ¥, how many were correct?_

### ğŸ” 2.Recall

```text
Recall = TP / (TP + FN)
```

â¡ï¸ _Out of all real ğŸ¥ in the test set, how many did we find?_

---

### ğŸ“ 3.F1 Score

> One number to rule them all â€“ balances precision vs recall

```text
F1 = 2 * (Precision * Recall) / (Precision + Recall)
```

ğŸ“Œ Great for **imbalanced datasets** (e.g. 90% cats, 10% dogs)

---

### ğŸ¤¯ Confusion Matrix

Hereâ€™s what your model really saw:

| Actual / Predicted | ğŸ© Donut | ğŸ¥¯ Bagel | ğŸ¥ Croissant |
| ------------------ | -------- | -------- | ------------ |
| ğŸ© Donut           | **15**   | 2        | 1            |
| ğŸ¥¯ Bagel           | 1        | **12**   | 3            |
| ğŸ¥ Croissant       | 0        | 2        | **11**       |

ğŸ§  This shows:

- It sometimes thinks **ğŸ¥ = ğŸ¥¯**
- Itâ€™s very good at detecting ğŸ©

Use it to **diagnose weaknesses** in your model.

---

### â™ Accuracy

> **"How often is my model correct?"**

```text
Accuracy = (TP + TN) / Total Predictions
```

ğŸ“Œ Problem: Doesnâ€™t tell you which class is doing poorly!

âœ… Use **only** when classes are balanced  
ğŸš« Donâ€™t trust it when classes are imbalanced

---

## ğŸ§  Macro vs Micro vs Weighted Averages

If youâ€™re working with **multi-class classification** (like above), Azure and AI-102 expect you to know this:

| Average Type | What It Means                                                              |
| ------------ | -------------------------------------------------------------------------- |
| **Micro**    | Global precision/recall across all classes (treats all equally)            |
| **Macro**    | Average of metrics per class (treats all classes equally)                  |
| **Weighted** | Average per class, _weighted by class size_ (accounts for class imbalance) |

---

## ğŸ“¦ Azure Custom Vision: How It Shows Metrics

After training a model:

- You'll see **per-tag Precision, Recall, and AP**
- Also a **macro-averaged F1 score**
- If you export the model performance, you get a CSV with:

  - **TagName**
  - **Precision**
  - **Recall**
  - **True Positives, False Positives, False Negatives**

ğŸ“Œ For **multiclass mode**, only **one tag is predicted per image**  
ğŸ“Œ For **multilabel mode**, **multiple tags** can be assigned per image

---

## ğŸ§ª Visual Summary

<div align="center">

```mermaid
graph TD
    A[Model Prediction] --> B{Correct?}
    B -- Yes --> C[True Positive]
    B -- No --> D{Actual Class?}
    D -- Positive --> E[False Negative]
    D -- Other --> F[False Positive]
```

</div>

---

## ğŸ§  When to Use What?

| Goal                             | Use This Metric          |
| -------------------------------- | ------------------------ |
| Want an overall score            | Accuracy                 |
| Want fewer false alarms          | Precision                |
| Want to catch every real example | Recall                   |
| Want a good balance              | F1 Score                 |
| Need per-class insights          | Confusion Matrix         |
| Have multiple labels per image   | Precision/Recall per tag |
| AI-102 exam?                     | **ALL of the above ğŸ˜„**  |

---

## ğŸ“Œ AI-102 Exam Tips

- Know the difference between Precision & Recall
- Understand confusion matrix for multiclass
- F1 = 2 \* P \* R / (P + R)
- Azure Custom Vision provides per-tag metrics
- Micro vs Macro averages are **often asked**

---

## ğŸ” Example Recap

Letâ€™s say for class â€œDogâ€:

- TP = 40
- FP = 10
- FN = 5

```text
Precision = 40 / (40 + 10) = 0.80
Recall    = 40 / (40 + 5) = 0.89
F1        = 2 * (0.8 * 0.89) / (0.8 + 0.89) â‰ˆ 0.84
```

---

## âœ… Summary Cheat Sheet

| Term             | Formula                  | Measures                  |
| ---------------- | ------------------------ | ------------------------- |
| Accuracy         | (TP + TN) / Total        | Overall correctness       |
| Precision        | TP / (TP + FP)           | How exact the model is    |
| Recall           | TP / (TP + FN)           | How complete the model is |
| F1 Score         | 2PR / (P + R)            | Balanced metric           |
| Confusion Matrix | Table of TP, FP, FN, etc | Class-specific analysis   |
