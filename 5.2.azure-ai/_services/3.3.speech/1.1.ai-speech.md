# ğŸ§  Azure AI Speech Service

Azure AI Speech empowers your apps to understand and generate human language via voice. It includes intelligent, real-time services like:

| ğŸ’¬ Feature              | ğŸ§  What It Does                                                       |
| ----------------------- | --------------------------------------------------------------------- |
| **Speech to Text**      | Converts spoken audio into text (aka speech recognition)              |
| **Text to Speech**      | Converts text into spoken audio (aka speech synthesis)                |
| **Speech Translation**  | Translates spoken language into one or more spoken or textual outputs |
| **Speaker Recognition** | Identifies or verifies a person based on their voice                  |
| **Custom Speech**       | Improves recognition for domain-specific terms or accents             |

> it also support **Keyword Recognition** which Listens for predefined keywords or commands  
> it also support **Intent Recognition** which Uses CLU (Conversational Language Understanding) to determine speaker's intent

---

## ğŸ§  Azure AI Speech = 3 Primary SDK Workflows

---

## 1ï¸âƒ£ Speech to Text (STT) ğŸ—£ï¸ â†’ ğŸ“

<div style="text-align: center;">
    <img src="images/speech-to-text-workflow.png" style="border-radius: 10px; width: 100%;" alt="speech-to-text-workflow">
</div>

### ğŸ” Workflow:

1. ğŸ§© **SpeechConfig** â€“ Contains subscription key & region info.
2. ğŸ™ï¸ **AudioConfig** â€“ Defines the source: microphone or `.wav` file.
3. ğŸ› ï¸ **SpeechRecognizer** â€“ The core recognizer (proxy client).
4. ğŸ§  **RecognizeOnceAsync()** â€“ Single-shot speech recognition.
5. ğŸ“¦ **SpeechRecognitionResult** â€“ Contains recognition output.

### ğŸ§ª SDK Output:

```csharp
var result = await recognizer.RecognizeOnceAsync();
Console.WriteLine($"Text: {result.Text}");
```

### ğŸ§¾ Result Properties:

| Property        | Meaning                                      |
| --------------- | -------------------------------------------- |
| `Text`          | The actual transcribed text.                 |
| `Duration`      | How long the spoken phrase lasted.           |
| `OffsetInTicks` | Timestamps (for aligning with video/audio).  |
| `ResultId`      | Unique operation ID.                         |
| `Reason`        | `RecognizedSpeech`, `NoMatch`, or `Canceled` |
| `Properties`    | Any metadata or custom diagnostic data       |

---

## 2ï¸âƒ£ Text to Speech (TTS) ğŸ“ â†’ ğŸ—£ï¸

<div style="text-align: center;">
    <img src="images/text-to-speech-workflow.png" style="border-radius: 10px; width: 100%;" alt="text-to-speech-workflow">
</div>

### ğŸ” Workflow:

1. ğŸ§© **SpeechConfig** â€“ Includes key, region, and voice settings.
2. ğŸ”ˆ **AudioConfig** â€“ Where to send audio: speaker, file, or stream.
3. ğŸ› ï¸ **SpeechSynthesizer** â€“ The proxy client for synthesis.
4. ğŸ” **SpeakTextAsync()** â€“ Converts input text into spoken output.
5. ğŸ“¦ **SpeechSynthesisResult** â€“ Contains output or error info.

### ğŸ§¾ Result Properties:

| Property     | Meaning                                        |
| ------------ | ---------------------------------------------- |
| `AudioData`  | Raw audio stream (can save/play)               |
| `Reason`     | `SynthesizingAudioCompleted`, `Canceled`, etc. |
| `ResultId`   | Request ID                                     |
| `Properties` | Metadata (voice used, latency, etc.)           |

### ğŸ§™ Voice Customization:

```csharp
speechConfig.SpeechSynthesisVoiceName = "en-GB-GeorgeNeural";
```

Azure supports over **400 neural voices** in **140+ languages**. Find full voice list: [https://learn.microsoft.com/en-us/azure/ai-services/speech-service/language-support](https://learn.microsoft.com/en-us/azure/ai-services/speech-service/language-support)

---

## 3ï¸âƒ£ Speech Translation ğŸ—£ï¸ â†’ ğŸŒ

<div style="text-align: center;">
    <img src="images/speech-translation-workflow.png" style="border-radius: 10px; width: 100%;" alt="speech-translation-workflow">
</div>

### ğŸ” Workflow:

1. ğŸ§© **SpeechTranslationConfig** â€“ Includes key, region, recognition language, target languages.
2. ğŸ¤ **AudioConfig** â€“ Input from mic or file.
3. ğŸŒ **TranslationRecognizer** â€“ Multilingual speech-to-text converter.
4. ğŸ” **RecognizeOnceAsync()** â€“ Transcribes & translates the spoken input.
5. ğŸ“¦ **TranslationRecognitionResult** â€“ Includes transcript & translations.

### ğŸ§¾ Result Properties:

| Property        | Meaning                                                      |
| --------------- | ------------------------------------------------------------ |
| `Text`          | Transcript of recognized speech (source)                     |
| `Translations`  | Dictionary with key = language code, value = translated text |
| `Duration`      | Length of utterance                                          |
| `OffsetInTicks` | Timestamp offset                                             |
| `Reason`        | `RecognizedSpeech`, `NoMatch`, `Canceled`                    |
| `ResultId`      | Unique ID                                                    |

---

## ğŸ” Speech-to-Speech Translation (ğŸ—£ï¸ â¡ï¸ ğŸŒ â¡ï¸ ğŸ—£ï¸)

You can **pipe translated text** into **Text to Speech** and get full spoken output:

```csharp
// Step 1: Translate speech
var result = await recognizer.RecognizeOnceAsync();
var translatedText = result.Translations["fr"];

// Step 2: Synthesize result
await synthesizer.SpeakTextAsync(translatedText);
```

### ğŸ§­ Two Ways to Translate and Synthesize:

| Scenario                 | Use this method               |
| ------------------------ | ----------------------------- |
| 1:1 translation          | Event-based synthesis         |
| Many-to-many translation | Manual synthesis per language |

---

## ğŸ§‘â€ğŸ¤ Speaker Recognition (Whoâ€™s Talking?)

Azure AI Speech provides **speaker recognition** capabilities through REST APIs and container support. It enables you to:

| Mode                       | What it Does                                                 |
| -------------------------- | ------------------------------------------------------------ |
| **Speaker Verification**   | Confirms if a voice matches a specific enrolled speaker      |
| **Speaker Identification** | Detects _who_ among a group of enrolled speakers is speaking |

### ğŸ¯ Use Cases:

- Voice-based login or access control (biometric auth)
- Personalization (adjust responses based on whoâ€™s talking)
- Forensic audio identification

### ğŸ” Workflow (Speaker Verification):

1. **Enroll the speaker** using a sample of their voice.
2. Use the `/verify` REST endpoint to compare new voice against stored profile.
3. Azure returns a **confidence score** and verification result.

### ğŸ§ª Example (Verify):

```http
POST /speaker/verify
Headers:
  Ocp-Apim-Subscription-Key: <your-key>
  Content-Type: audio/wav
Body: <audio sample>
```

```json
{
  "recognitionResult": "Accept",
  "score": 0.98,
  "profileId": "xyz-123"
}
```

---

## ğŸ§  Custom Speech (Train for Accents, Jargon, or Noisy Mics)

Custom Speech allows you to improve **speech recognition accuracy** for:

- Industry-specific jargon (e.g., "cardiotocography")
- Brand names or acronyms (e.g., "Contoso CRM")
- Non-native accents or background noise

### ğŸš€ Workflow:

1. Upload training data (audio + transcripts) to **Speech Studio**.
2. Create and train a **Custom Speech model**.
3. Deploy the model and get the **custom endpoint**.
4. Use `SpeechConfig.endpoint = "<custom endpoint>"` in SDKs.

### ğŸ§ª Sample:

```csharp
speechConfig.EndpointId = "your-custom-speech-model-id";
var recognizer = new SpeechRecognizer(speechConfig, audioConfig);
```

âœ… Works exactly like normal recognition â€“ just more accurate for your domain!

---

## âš™ï¸ Configure audio format and voices

### ğŸ”‰ Audio format

The Azure AI Speech service supports multiple output formats for the audio stream that is generated by speech synthesis. Depending on your specific needs, you can choose a format based on the required:

- Audio file type
- Sample-rate
- Bit-depth

### ğŸ—£ï¸ Voices

The Azure AI Speech service provides multiple voices that you can use to personalize your speech-enabled applications. There are two kinds of voice that you can use:

Standard voices - synthetic voices created from audio samples.
Neural voices - more natural sounding voices created using deep neural networks.

```c#
speechConfig.SpeechSynthesisVoiceName = "en-GB-George";
```

## ğŸ¨ SSML â€“ Speech Synthesis Markup Language

SSML = HTML for speech.

```xml
<speak version="1.0" xmlns="http://www.w3.org/2001/10/synthesis"
       xmlns:mstts="https://www.w3.org/2001/mstts" xml:lang="en-US">
  <voice name="en-US-AriaNeural">
    <mstts:express-as style="cheerful">Let's celebrate success!</mstts:express-as>
  </voice>
  <break time="500ms"/>
  <voice name="en-US-GuyNeural">
    I say <phoneme alphabet="sapi" ph="t ao m ae t ow"> tomato </phoneme>.
  </voice>
</speak>
```

ğŸ”¹ Use **express-as**, **break**, **phoneme**, **say-as**, etc.

---

## ğŸ§  Azure Speech Studio

> ğŸ”— [Azure Speech Studio](https://speech.microsoft.com/)

The no-code web UI for:

- Testing STT, TTS, and translation
- Creating and training **Custom Speech**
- Creating voice assistants
- Testing pronunciation

---

## ğŸ§° Bonus Features

| ğŸ”§ Feature                     | âš™ï¸ What It Does                                                          |
| ------------------------------ | ------------------------------------------------------------------------ |
| **Custom Speech**              | Improve recognition accuracy for noisy/technical/industry-specific audio |
| **Real-time Transcription**    | Continuous recognition, intermediate results, low latency                |
| **Batch Transcription**        | Useful for large .wav/.mp3 files in Azure Blob Storage                   |
| **Audio Format Customization** | Choose sample rate, channel layout, bit depth                            |

---

## âœ… AI-102 Exam Tips

- Know difference between `SpeechRecognizer`, `TranslationRecognizer`, `SpeechSynthesizer`.
- Understand result properties like `Text`, `Reason`, `ResultId`, `Translations`.
- SSML gives you **more control** over the speech outputâ€”memorize key tags.
- `AudioConfig` can be mic input (real-time) or file input/output.
- Translate then synthesize = manual speech-to-speech translation pipeline.

Absolutely! Below are **three clean, structured, and SDK-aligned sections** for the Azure AI Speech features you asked about. These will integrate well with the rest of your enhanced documentation:
