# 🧠 Prompt Parameters in Azure OpenAI Studio

These parameters control **how the model thinks, talks, and behaves** during inference (when generating responses).

Let’s go one by one, with examples, metaphors, and practical tips:

---

## ✅ Summary Table

| Parameter             | What It Does                         | Good For                          |
| --------------------- | ------------------------------------ | --------------------------------- |
| **System Message**    | Sets the AI’s role or tone           | Character/personality tuning      |
| **Past Messages**     | How much chat history AI remembers   | Chatbots, continuity              |
| **Max Response**      | Max length of reply                  | Cost control, formatting          |
| **Temperature**       | Controls creativity                  | Creative writing, story, poems    |
| **Top P**             | Limits randomness (nucleus sampling) | Factual or focused answers        |
| **Stop Sequence**     | Ends generation at a certain word    | API responses, structured outputs |
| **Frequency Penalty** | Prevents repeated **words**          | Summary, blog, doc generation     |
| **Presence Penalty**  | Prevents repeated **ideas**          | Brainstorming, topic generation   |

---

## 🧩 1. **System Message** (🧙 Prompt Personality)

> Controls: The _role_, behavior, or tone of the assistant  
> Example: `"You are a helpful assistant"`

### 📘 What is it?

It’s like **setting the character and behavior** of the assistant **before** the conversation starts.

### 🧠 Analogy:

You’re hiring an AI. The **system message** is the **job description + personality** you assign:

- “You are a lawyer”
- “You are sarcastic”
- “You respond only with bullet points”
- “You are a pirate speaking only pirate slang”

### 🔥 Example:

```json
"You are a helpful assistant that always answers in haiku format."
```

> The system message never shows in output, but it influences **everything** the AI says.

---

## 🕰️ 2. **Past Messages Included** (💬 Memory Window)

> Controls: How many prior Q\&As the model remembers per request.  
> Example: `5` messages means 5 previous turns (user + AI) are included.

### 📘 What is it?

This controls **how much context** from the past the model uses for answering your current question.

### 🧠 Analogy:

Think of this like the AI’s **short-term memory**. You can say:

- “Forget everything, only care about my current question”
- Or: “Remember the last 5 things I said”

### 🔥 Tip:

- In **RAG apps**, a smaller context (2–5) helps performance.
- For **chatbots**, higher context (8–10) improves continuity.

---

## 📏 3. **Max Response Tokens** (🧮 How Long Should the Answer Be?)

> Controls: Maximum **length** of AI's response
> Example: `500 tokens` ≈ 350–400 words.

### 📘 What is it?

This limits how **long** the response can be (in tokens, not characters).

> ⚠️ One token ≈ 4 characters ≈ 0.75 words (in English)

### 🧠 Analogy:

You’re writing an article, and the editor says, “You get 200 words max.” That’s what this does.

### 🔥 Tip:

- Short answers? Use 50–100
- Detailed summaries or documents? 500–1000
- Watch your quota! More tokens = 💸 more \\\\\\\\\\\\\\\\\\\\\$\\\\\\\\\\\\\\\\\\\\\$\$

---

## 🔥 4. **Temperature** (🎲 Creativity Level)

> Controls: How **random** or **creative** the AI is
> Range: 0.0 – 2.0 (default: **0.7**)

### 📘 What is it?

Higher temperature = More _creative_, _random_, or _risky_ answers
Lower = More _factual_, _precise_, _safe_

### 🧠 Analogy:

It’s like how free the AI is allowed to think:

- `0.0` = “Just the facts, please”
- `1.0` = “Be creative, explore ideas”
- `2.0` = “You're drunk, write a fairytale!”

### 🔥 Example:

| Prompt                          | Response (Temp 0.0) | Response (Temp 1.5)            |
| ------------------------------- | ------------------- | ------------------------------ |
| "What’s the capital of France?" | "Paris"             | "The baguette kingdom, Paris!" |

---

## 🎯 5. **Top P (Nucleus Sampling)**

> Controls: Another way to **limit randomness**  
> Range: 0.1 – 1.0

### 📘 What is it?

Top P tells the model to only consider the **top X% most likely words** during generation.

> Example: Top P = 0.9 → Only use words that together make up 90% of probability mass.

### 🧠 Analogy:

Like **choosing from a shorter menu** of possible words to say.

### 🔥 Tip:

- **Top P = 1.0** → Consider all possible words
- **Top P = 0.5** → Narrowed, focused generation
- Works best when tuned **with temperature**

---

## 🛑 6. **Stop Sequences** (🚫 End of Generation)

> Controls: What **word or character** makes the model stop replying  
> Example: `"###"` or `"The End"`

### 📘 What is it?

It defines **when to stop generating** tokens — like a punctuation mark the model obeys.

### 🧠 Analogy:

It’s like a teacher saying, “When I say ‘Stop’, stop reading aloud.”

### 🔥 Use Cases:

- Ending code generation: `\n\n`
- Finishing multi-part answers: `"---"`
- Prevent long rambles

---

## 🔁 7. **Frequency Penalty** (♻️ Don’t Repeat Too Much)

> Controls: Reduce repetition of exact same **words**  
> Range: 0.0 – 2.0

### 📘 What is it?

This penalizes the model from repeating **the same tokens** over and over again.

### 🧠 Analogy:

If the model keeps saying "Paris is the capital of France" 5 times — this fixes it.

### 🔥 Tip:

- Set to **0.5–1.0** if your output is **repetitive**
- For story generation, helps keep content diverse

---

## 🎭 8. **Presence Penalty** (🚫 Don’t Talk About the Same Stuff)

> Controls: Discourage repetition of **topics or themes**  
> Range: 0.0 – 2.0

### 📘 What is it?

It penalizes the model if it tries to talk about a topic **it already mentioned** — keeps conversations moving.

### 🧠 Analogy:

You ask “Tell me about AI” and it keeps going back to “ChatGPT” in every answer. Presence penalty says, “Let’s talk about something **new**!”

### 🔥 Tip:

- Use **0.5–1.0** when generating multiple ideas (e.g. headlines, topics, FAQs)
- Great for **creative writing**, brainstorming

---

## 🧪 Example Prompt Configuration (for a polite helper)

```json
{
  "model": "gpt-35-turbo",
  "system_message": "You are a polite AI assistant that speaks in clear short answers.",
  "temperature": 0.5,
  "top_p": 0.9,
  "max_tokens": 300,
  "presence_penalty": 0.6,
  "frequency_penalty": 0.3,
  "stop_sequences": ["###"],
  "messages": [
    {
      "role": "user",
      "content": "What is the capital of France?"
    }
  ]
}
```
