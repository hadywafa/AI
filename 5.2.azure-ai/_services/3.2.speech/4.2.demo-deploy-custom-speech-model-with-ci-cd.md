# ğŸš€ Deploying a Custom Speech Model using CI/CD

Welcome to the most **epic showdown** between "Manual Click-Ops" and "Automated Deployment Glory." In this topic, youâ€™ll learn how to **deploy a trained Azure Custom Speech Model using CI/CD pipelines** in GitHub Actions. Weâ€™ll walk through everything from repo setup to automating training, testing, and deployment â€” with a real, working example.

---

## ğŸ§  Whatâ€™s This About?

We previously learned how to train a custom speech model in **Azure Speech Studio** using dinosaur names (yep ğŸ¦•). Now, letâ€™s automate it using **MLOps concepts** and GitHub Actions so that it:

- ğŸ¯ Tests model quality
- ğŸ§ª Trains the model
- âœ… Compares old vs new accuracy
- ğŸ“¦ Deploys only if the new model is better
- ğŸ” Can be reused for future speech projects (like product names, cities, medical terms)

---

## ğŸ” CI/CD Pipeline Summary (CI vs CD)

| Step   | CI (Continuous Integration)     | CD (Continuous Deployment)                          |
| ------ | ------------------------------- | --------------------------------------------------- |
| Goal   | Test accuracy of baseline model | Train new model, test it, and deploy if it's better |
| Input  | Test data (audio + transcript)  | Training data (e.g. list of words), test data       |
| Output | Error rate (e.g. 20%)           | Deployment of improved model (e.g. 3% error)        |

---

## ğŸ—ƒï¸ Folder Structure in the GitHub Repo

GitHub Repo: [Azure-Samples/Speech-Service-Actions-Template](https://github.com/Azure-Samples/Speech-Service-Actions-Template)

```ini
ğŸ“ .github/workflows/
â”‚   â”œâ”€â”€ speech-test-data-ci.yml    # CI - benchmark testing
â”‚   â””â”€â”€ speech-train-deploy-cd.yml # CD - training and deploying
ğŸ“ test-data/
â”‚   â””â”€â”€ test/                      # Contains audio + transcript (benchmark)
ğŸ“ training-data/
â”‚   â”œâ”€â”€ language.txt               # Words to teach (e.g. "GoDrive", "Sharjah")
â”‚   â””â”€â”€ pronunciation.txt          # Optional phonetic mappings
```

---

## ğŸ› ï¸ CI Pipeline: speech-test-data-ci.yml

This pipeline calculates how â€œbadâ€ the **baseline model** is. Think of it like a school test:

- ğŸ§ Give the model an audio file ("GoDrive")
- ğŸ“„ Compare its transcription with a correct one
- ğŸ“Š Measure word error rate (WER) â€” e.g. 19.6%
- ğŸ’¾ Save this benchmark in Azure Blob Storage

### ğŸ§© Key Steps:

- **Login to Azure**
- **Upload test data (audio + transcript)**
- **Run `spx csr evaluate` command via CLI**
- **Save results in Blob Storage**

> ğŸ“Œ This pipeline does not train the model â€” it only evaluates the default one.

---

## ğŸš€ CD Pipeline: speech-train-deploy-cd.yml

Now letâ€™s _train_, _test_, and _deploy_ if the model is better.

### ğŸ§© Key Steps:

1. **Login to Azure**
2. **Upload training data** (`language.txt`, `pronunciation.txt`)
3. **Train the model** using:

   ```bash
   spx csr model create --name "newModel"
   ```

4. **Test the model** using same test data:

   ```bash
   spx csr evaluation create --model-id newModel
   ```

5. **Compare the WER** with previous benchmark
6. **If better (e.g., 1.7% vs 19.6%)**, deploy model using:

   ```bash
   spx csr endpoint create --model-id newModel
   ```

---

## ğŸ” Secrets & Setup in GitHub Actions

You need the following GitHub Secrets to authenticate and trigger pipelines:

| Secret Name             | Purpose                             |
| ----------------------- | ----------------------------------- |
| `AZURE_CREDENTIALS`     | Service principal JSON for login    |
| `SPEECH_KEY`            | Azure Speech API key                |
| `SPEECH_REGION`         | Azure region (e.g., `eastus`)       |
| `AZURE_STORAGE_ACCOUNT` | Azure Blob Storage for test results |
| `PROJECT_NAME`          | Name of your Speech project         |

> These are used by the pipeline to access Azure, Speech Studio, and storage securely.

---

## ğŸ§  How It Knows What to Do?

Both workflows use a CLI called **Speech CLI (SCLI)** which mimics all the clicks youâ€™d do in Speech Studio. It's called via this command:

```bash
spx csr model create --project-name "YourProject" --language-en "language.txt"
```

And evaluated using:

```bash
spx csr evaluation create --model-id your_model_id --dataset "test_dataset"
```

---

## ğŸ”¬ Sample Result (From Blob)

| Model      | WER (Word Error Rate)   |
| ---------- | ----------------------- |
| Baseline   | 19.6% (before training) |
| Trained V2 | 1.7% âœ…                 |

ğŸ“‚ Stored in Azure Blob Storage for future comparison and tracking.

---

## ğŸ§ª When Will It Run?

- âœ… **CI runs** when you push a **Git tag** like `baseline-1`
- âœ… **CD runs** when you **trigger it manually** (or you can automate it via PRs or commits)

---

## ğŸ‘€ GitHub Logs: Peek Inside

You can inspect each GitHub Actions step live:

1. Go to your repo â†’ **Actions tab**
2. Click on the workflow â†’ **Expand steps**
3. See logs, outputs, even SPX commands that ran

Example:

```bash
spx csr evaluation create --model-id model123 --dataset testData123
```

---

## ğŸ§¼ Optional Cleanup (or Not)

You can optionally **delete models and data** at the end of the pipeline to save cost. Or **comment out** cleanup steps in shell script like:

```bash
spx csr evaluation delete --evaluation-id ...
```

> âš ï¸ Speech training is **expensive** â€” plan for \~\$50â€“\$100 per run on Standard tier.

---

## ğŸ’¡ Summary: What You Just Automated

- âœ… Train custom speech model (from CLI)
- âœ… Evaluate both old & new model
- âœ… Compare results programmatically
- âœ… Deploy only when model improves
- âœ… Store everything in Azure for visibility
