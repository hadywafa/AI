# ğŸ–¼ï¸ Azure AI Content Safety â€“ Image Analysis with Python

## ğŸ§¯ Why Bother Moderating Images?

You let users upload images on your platform?
Congrats, you also opened the door to ğŸ” memes, ğŸ§¨ hate symbols, ğŸ©¸ gore, and all sorts of digital chaos.
Thatâ€™s why **image moderation** is critical â€” for **trust**, **compliance**, and **user safety**.

Luckily, Azure gives you a **fully managed AI solution** to detect:

| Category         | Examples                                               |
| ---------------- | ------------------------------------------------------ |
| ğŸ§¨ **Hate**      | Racist flags, hate speech in images, offensive symbols |
| ğŸ©¸ **Self-harm** | Razor blades, blood, self-injury photos                |
| ğŸ‘ **Sexual**    | Nudity, adult content, sexually suggestive imagery     |
| âš”ï¸ **Violence**  | Guns, fights, blood, injuries                          |

Each detection comes with a **severity level from 0 (safe) to 6 (extreme)**.

---

## âš™ï¸ What You Need Before Coding

âœ… Azure subscription
âœ… A **Content Safety** resource (create from Azure Portal)
âœ… Access to its `key` and `endpoint`
âœ… Install the SDK locally:

```bash
pip install azure-ai-contentsafety
```

---

## ğŸ§ª Full Working Code â€“ Analyze Any Image

Here's the complete Python script using Azure's SDK to analyze image content:

```python
# Azure AI ContentSafety Image Analysis
# pip install azure-ai-contentsafety

import os
from azure.ai.contentsafety import ContentSafetyClient
from azure.ai.contentsafety.models import (
    ImageCategory, AnalyzeImageOptions, ImageData
)
from azure.core.credentials import AzureKeyCredential
from azure.core.exceptions import HttpResponseError

# ğŸ” Your real Azure AI key + endpoint
key = "f7324ffadc7e4da0b5b253230a44a849"
endpoint = "https://azure-content-safety-demo-345.cognitiveservices.azure.com/"

# ğŸ“¸ Image path (local file)
image_path = os.path.abspath(
    os.path.join(os.path.abspath(__file__), "..", "./sample_data/756692568a236c94619b202e9b68687a.jpg")
)

# ğŸ§  Create the Content Safety client
client = ContentSafetyClient(endpoint, AzureKeyCredential(key))

# ğŸ§¾ Read and prepare image for API
with open(image_path, "rb") as file:
    request = AnalyzeImageOptions(image=ImageData(content=file.read()))

# ğŸ” Analyze the image
try:
    response = client.analyze_image(request)
except HttpResponseError as e:
    print("Analyze image failed.")
    if e.error:
        print(f"Error code: {e.error.code}")
        print(f"Error message: {e.error.message}")
    raise

# ğŸ“Š Extract and display results
hate_result = next(item for item in response.categories_analysis if item.category == ImageCategory.HATE)
self_harm_result = next(item for item in response.categories_analysis if item.category == ImageCategory.SELF_HARM)
sexual_result = next(item for item in response.categories_analysis if item.category == ImageCategory.SEXUAL)
violence_result = next(item for item in response.categories_analysis if item.category == ImageCategory.VIOLENCE)

if hate_result:
    print(f"ğŸ§¨ Hate severity: {hate_result.severity}")
if self_harm_result:
    print(f"ğŸ©¸ SelfHarm severity: {self_harm_result.severity}")
if sexual_result:
    print(f"ğŸ‘ Sexual severity: {sexual_result.severity}")
if violence_result:
    print(f"âš”ï¸ Violence severity: {violence_result.severity}")

if __name__ == "__main__":
    analyze_image()
```

---

## ğŸ“ˆ Sample Output

Letâ€™s say the image was a mildly suggestive political meme. You might see:

```ini
ğŸ§¨ Hate severity: 1
ğŸ©¸ SelfHarm severity: 0
ğŸ‘ Sexual severity: 3
âš”ï¸ Violence severity: 0
```

Now **you can decide**:

- **0â€“1**: Auto-approve âœ…
- **2â€“4**: Send for human review ğŸ•µï¸â€â™€ï¸
- **5â€“6**: Block or report ğŸš¨

---

## ğŸ“½ï¸ Behind-the-Scenes Flow (Mermaid Diagram)

<div align="center">

```mermaid
sequenceDiagram
    participant Dev as Your Python App
    participant SDK as Azure SDK
    participant API as Azure Content Safety API
    participant AI as AI Model

    Dev->>SDK: Create AnalyzeImageOptions
    SDK->>API: Send image to /analyzeImage
    API->>AI: Classify image by category
    AI-->>API: Return severity per category
    API-->>SDK: Structured JSON response
    SDK-->>Dev: Expose .categories_analysis
```

</div>

---

## ğŸ§µ Explanation of Key Parts

| Line/Concept               | Description                                    | Emoji    |
| -------------------------- | ---------------------------------------------- | -------- |
| `AnalyzeImageOptions`      | Tells API what image to check                  | ğŸ“¤       |
| `ImageData(content=...)`   | Encodes image binary data into payload         | ğŸ§¾       |
| `client.analyze_image()`   | Main function call to Content Safety API       | ğŸ¤–       |
| `ImageCategory.HATE`, etc. | Enum values to filter result categories        | ğŸ§¨ğŸ©¸ğŸ‘âš”ï¸ |
| `item.severity`            | Score between 0â€“6 indicating content intensity | ğŸ”¥       |

---

## ğŸ’¡ Smart Usage Tips

- ğŸ›‘ Don't hard-block based only on **keywords or tags** â€” use severity + business logic.
- ğŸ§ª Test with real-world uploads from your users â€” memes, screenshots, camera photos.
- ğŸ”’ Store flagged images temporarily for **human review or appeal**.
- ğŸš€ Use this in a background moderation pipeline or even **serverless with Azure Functions**.
- ğŸ§¼ Combine with **text moderation**, profanity filters, or LLM-based content summarizers.

---

## ğŸ“Š Summary Table

| Feature              | Value                                    |
| -------------------- | ---------------------------------------- |
| Supported formats    | JPG, PNG, BMP, TIFF                      |
| Max file size        | 4MB                                      |
| Categories supported | Hate, Self-Harm, Sexual, Violence        |
| Severity scale       | `0` (safe) â†’ `6` (extreme content)       |
| SDK                  | `azure-ai-contentsafety`                 |
| Call type            | Sync (`analyze_image()`)                 |
| Result format        | `.categories_analysis`                   |
| Cost                 | Pay-per-API-call, Free tier available ğŸ’° |

---

## ğŸ“ Official Resources

- [ğŸ“– Azure Docs â€“ Analyze Image](https://learn.microsoft.com/en-us/azure/ai-services/content-safety/how-to/image-moderation)
- [ğŸ”§ REST API Reference](https://learn.microsoft.com/en-us/rest/api/cognitiveservices/content-safety/analyze-image)
- [ğŸ§° SDK on PyPI](https://pypi.org/project/azure-ai-contentsafety/)
- [ğŸ§  Responsible AI Guidelines](https://learn.microsoft.com/en-us/azure/responsible-ai/overview)
