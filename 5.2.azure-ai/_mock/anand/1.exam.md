# Exam 1

## âŒ Question 6

![ex1_q6](images/ex1_q6.png)

---

**âœ… The answer:**
`1. https://centralus.api.cognitive.microsoft.com`
`2. /text/analytics/v3.1/languages`

---

**ğŸ¤” Why This Is the Best Answer:**

- `https://centralus.api.cognitive.microsoft.com`: This is a valid **base endpoint** for Azure Cognitive Services in the **Central US region**, commonly used for services like Text Analytics (Language Detection).
- `/text/analytics/v3.1/languages`: This is the correct **API path** for detecting the language of a given input using the **Azure AI Language** service. It helps you determine whether a customer query is in English or Spanish so you can route it accordingly.

---

**âŒ Why Other Options Are Wrong:**

- `https://eu.api.cognitive.microsofttranslator.com`: While valid for Translator, it doesn't relate to **language detection**; it's just a **regional base URL**.
- `/translator/text/v3.0/translate?to=es` and `/translate?to=en`: These are **translation endpoints**, not for detecting language. Translation is **not required** in this use case since the goal is routing based on **original message language**, not converting it.
- `https://portal.azure.com`: This is just the **web portal**, not an API endpoint. It cannot be used in code to detect or translate language.

---

## âŒ Question 12

![ex1_q12](images/ex1_q12.png)

---

**âœ… The answer:**

- **A. Use the Language Studio's performance dashboard to view metrics and evaluation reports.**
- **C. Retrieve the evaluation report from the authoring API provided by the language service.**

---

**ğŸ¤” Why This Is the Best Answer:**
These two methods directly support **pre-deployment evaluation**:

- âœ… **A. Language Studioâ€™s performance dashboard** gives you an interactive view of how your model performs on test data â€” showing **metrics like precision, recall, F1 score**, confusion matrices, and misclassified utterances.

- âœ… **C. Authoring API for evaluation summary** allows you to **retrieve detailed evaluation reports** via REST. You can use this in automation or integrate it into your CI/CD pipeline.

Example API:

```http
GET https://{endpoint}/language/authoring/analyze-conversations/projects/{projectName}/models/{trainedModelLabel}/evaluation/summary-result?api-version=2023-04-01
```

---

**âŒ Why Other Options Are Wrong:**

- âŒ **B. Enable feedback collection...**
  Feedback collection is part of a **post-deployment** improvement loop. It relies on **live user interaction logs** and does **not help in pre-deployment** evaluation.

- âŒ **D. Azure Monitor**
  Azure Monitor is used for **infrastructure and service health**, not for evaluating the **model's linguistic understanding**. It tracks **performance metrics like latency and availability**, not NLP accuracy.

---

ğŸ“˜ **Key Concept (AI-102):**

> **Pre-deployment evaluation** should be done using test datasets, evaluation reports, and built-in performance dashboards â€” **not production feedback or monitoring tools**.

---

## âŒ Question 15

![ex1_q15](images/ex1_q15.png)

---

**âœ… The answer:**

- **A. to=el**
- **C. toScript=Latn**
- **E. textType=html**

---

**ğŸ¤” Why This Is the Best Answer:**

- âœ… **A. `to=el`** â€“ This sets the **target language to Greek**, which is the main translation objective. `"el"` is the correct ISO language code for Greek.

- âœ… **C. `toScript=Latn`** â€“ This enables **Roman (Latin script) transliteration** of the translated Greek text. Greek normally uses the Greek script, so `toScript=Latn` provides a phonetic version readable by non-Greek users.

- âœ… **E. `textType=html`** â€“ This indicates the input is **HTML content**, so the API treats tags appropriately and doesnâ€™t translate elements like `<div>` or `<strong>`. This is important for product descriptions that may contain formatting.

---

**âŒ Why Other Options Are Wrong:**

- âŒ **B. `from=fr`** â€“ This is only useful if the source text is explicitly in French. Since source detection is automatic by default and the question doesnâ€™t mention French, this is unnecessary.

- âŒ **D. `textType=xml`** â€“ This is not correct for standard product description text. Unless you're passing actual XML documents, `textType=html` is more appropriate.

- âŒ **F. `toScript=Cyrl`** â€“ `Cyrl` is Cyrillic script (used in Russian, Serbian, etc.), which is **not appropriate for Greek**. Greek uses its own script and transliterates to **Latin (Latn)**, not Cyrillic.

---

## âŒ Question 16

![ex1_q16](images/ex1_q16.png)

---

**âœ… The answer:**
**(A) 1, 4 â€” `StorageConnectionString`, `Objects`**

---

**ğŸ¤” Why This Is the Best Answer:**
To build a functional **Knowledge Store** in Azure AI Search that supports **enriched data like Sentiment Analysis**, you need to define:

- âœ… **`StorageConnectionString`** â€“ This is **mandatory** to connect your Knowledge Store to an **Azure Storage account** where enriched data (tables, objects) will be saved.
- âœ… **`Objects`** â€“ This defines **projection formats** such as enriched JSON documents. Objects are used when you want to store complete enriched documents (e.g., customer review + sentiment + key phrases) for flexible retrieval or further processing.

These two together allow:

- Saving enriched results in a structured but flexible way.
- Retrieving entire enriched documents (great for dashboards, analytics, or full reviews).

---

**âŒ Why Other Options Are Wrong:**

- âŒ **`Tables`** â€“ While valid, itâ€™s optional and only needed when you want **normalized/tabular projections** (like splitting key phrases or entities into separate rows). Itâ€™s not the best **default** if you want flexible, full-document access.

- âŒ **`ContainerName`** â€“ This is **not a valid field** in the `knowledgeStore` definition block. Azure uses the storage connection string to resolve the correct container or blob context.

- âŒ **C) 1, 2 (`StorageConnectionString`, `Tables`)** â€“ Close, but not ideal for **flexible access**. Tables are great for relational processing, not for retrieving the entire enriched review.

- âŒ **B) 3, 2 (`ContainerName`, `Tables`)** â€“ Incorrect. `ContainerName` is not used in Knowledge Store definitions. Plus, it omits the required `StorageConnectionString`.

---

ğŸ“¦ **Correct Knowledge Store JSON (simplified):**

```json
"knowledgeStore": {
  "storageConnectionString": "<your-azure-blob-connection-string>",
  "projections": [
    {
      "objects": [
        {
          "source": "/document",
          "storageContainer": "enriched-reviews"
        }
      ]
    }
  ]
}
```

---

## âœ… Question 20

![ex1_q20](images/ex1_q20.png)

**âœ… The answer:**
**C) Change Domains to General (compact) â†’ Retrain the model â†’ Export the model**

---

**ğŸ¤” Why This Is the Best Answer:**
To export a model for **offline or edge deployment**, Azure Custom Vision requires that you use a **Compact domain** (like _General (compact)_), which supports model export in ONNX, TensorFlow, or other embedded formats.

Hereâ€™s why each step is necessary:

1. âœ… **Change Domains to General (compact):**

   - Only **Compact domains** support **exporting models**. The standard "General" domain does **not support export**.

2. âœ… **Retrain the model:**

   - After changing the domain, you **must retrain** the model â€” Custom Vision models are tied to a specific domain during training.

3. âœ… **Export the model:**

   - Once retrained with a compact domain, the **export button becomes available**, allowing you to download the model in formats like ONNX or TensorFlow Lite for use on devices without internet access.

---

**âŒ Why Other Options Are Wrong:**

- âŒ **A. Optimize model for edge deployment** â€“ Thereâ€™s no separate "optimize" step in Custom Vision. The optimization happens automatically when you **train using a compact domain**.

- âŒ **B. Retrain the model** (first) â€“ Retraining before switching to a compact domain is **pointless**, because only compact domains allow exports. Youâ€™d have to retrain again anyway after switching.

- âŒ **D. Change the classification type** â€“ This isnâ€™t related to exportability. It determines **how labels are assigned** (multiclass vs. multilabel), not deployment compatibility.

- âŒ **E. Create a new classification model** â€“ You can reuse the existing project by just changing its domain and retraining. Creating a new model isnâ€™t necessary unless you want to start from scratch.

---

ğŸ› ï¸ **Pro Tip:**
For exporting a model to work on disconnected or embedded environments like Raspberry Pi, microcontrollers, or industrial devices, **always use Compact domains** in Custom Vision.

---

ğŸ“˜ **Final Workflow:**
ğŸ‘‰ **Change to General (compact)** â†’ ğŸ” **Retrain the model** â†’ â¬‡ï¸ **Export the model for offline use**.

---

## âŒ Question 29

![ex1_q29](images/ex1_q29.png)

---

![ex1_q29_answer](images/ex1_q29_answer.png)

Why other options are not correct:

No logging options are mentioned in the docker run command. For ex:

![ex1_q29_answer_explain](images/ex1_q29_answer_explain.png)

---

## âŒ Question 44

![ex1_q44](images/ex1_q44.png)

---

**âœ… The answer:**
**Jailbreak risk detection**

---

**ğŸ¤” Why This Is the Best Answer:**

- **Jailbreak risk detection** is a **specialized safety feature** in Azure OpenAI that helps detect **attempts to bypass or manipulate content filters**.
- It identifies **malicious prompts** designed to get the model to output harmful, unsafe, or restricted content (e.g., asking the model to "pretend" or "ignore safety rules").
- This is **critical for protecting conversational AI apps** that rely on OpenAI models, especially before or alongside traditional moderation techniques.

---

**âŒ Why Other Options Are Wrong:**

- âŒ **Moderate text content** â€“ This is part of **Azure AI Content Safety**, which detects **harmful, sexual, hate, or violent language**, but it **doesn't detect jailbreak attempts** that try to trick the model without using explicit harmful content.

- âŒ **Protected material detection** â€“ This is for detecting **copyrighted content**, such as music, code, or text fragmentsâ€”not applicable to **malicious intent** in prompts.

- âŒ **Groundedness detection** â€“ This checks if an LLMâ€™s answer is **factually supported** by source documents (used in RAG apps). It doesn't identify **user prompt manipulation or jailbreaks**.

---

ğŸ“˜ **Pro Tip for AI-102 & Real Apps:**
Use **Jailbreak risk detection** for:

- Guarding against **prompt injection**
- Blocking **model manipulation** attempts
- Working alongside **content moderation** and **user input filters** for robust safety

ğŸ” Combine it with **Content Safety APIs** for a **layered defense strategy** in production apps.

---

## âŒ Question 45

![ex1_q45_1](images/ex1_q45_1.png)
![ex1_q45_2](images/ex1_q45_2.png)

---

**âœ… The answer:**
**None of the above**

---

**ğŸ¤” Why This Is the Best Answer:**

Letâ€™s break down the statements and explain why **each one is false** based on the response JSON and model settings:

---

ğŸ” Statement: _"The OpenAI model used is text-embedding-ada-002"_

**âŒ False!**

- The model clearly specified in the response is:
  `"model": "gpt-3.5-turbo-0301"`
- `text-embedding-ada-002` is used **only for embeddings**, not for chat or completions.

---

ğŸ” Statement: _"The prompt_tokens value will be included in the calculation of the Max response tokens value."_

**âŒ False!**

- `max_tokens` (or max response tokens) applies **only to the output tokens** (completion).
- The prompt (input) token count is not included in this cap.
  ğŸ‘‰ In this case:

  - `prompt_tokens = 37`
  - `completion_tokens = 86`
  - `max_response_tokens = 100` â†’ the model was allowed to generate up to 100 **output** tokens.

---

ğŸ” Statement: _"The subscription will be charged 86 tokens for the execution of the session."_

**âŒ False!**

- Billing in Azure OpenAI is based on **total tokens** used:

  - `prompt + completion = 37 + 86 = 123 tokens`

- So you are charged for **123 tokens**, not just the output.

---

ğŸ” Statement: _"The text completion was cut off due to exceeding the Max response tokens limit."_

**âŒ False!**

- The setting allows for up to **100 response tokens**, and the model only used **86**, well within the limit.
- Also, the `"finish_reason": "stop"` confirms the model ended naturally.
  If it had hit the max token cap, it wouldâ€™ve said: `"finish_reason": "length"`

---

**âœ… Summary:**
None of the options are true based on the model used, token accounting, billing logic, or how completions are handled.

---

ğŸ“˜ **Pro Tip (AI-102 or devs):**
To avoid surprises in billing and truncation:

- Always monitor both `prompt_tokens` and `completion_tokens`
- Use `"finish_reason"` to programmatically handle truncated responses.

---

## âœ… Question 46

![ex1_q46](images/ex1_q46.png)

---

**âœ… The answer:**
**B) An API key, A deployment name**

---

**ğŸ¤” Why This Is the Best Answer:**

To allow three different apps to access **three separate deployments** of the GPT-3.5 model under the same Azure OpenAI resource, the setup should be:

---

### ğŸ” **1. Provide access to AIDev by using: An API key**

- âœ… Azure OpenAI uses **API keys** for authenticating requests made via REST API.
- Each request must include this key in the `Ocp-Apim-Subscription-Key` header.
- This enables easy control and avoids needing Azure AD integration for typical use.

ğŸ›‘ **Why others are wrong:**

- **Bearer token / AD token:** These are used for **Azure AD-based** access, not commonly required or configured for OpenAI resources unless using RBAC and managed identities, which isnâ€™t the default method for REST-based access.

---

### ğŸ”— **2. Connect to the deployment by using: A deployment name**

- âœ… Even though all three deployments share the same **resource endpoint**, each one has a unique **deployment name** (e.g., `gpt-chat-support`, `gpt-recommendation`, `gpt-content-gen`).
- You must include this **deployment name** in the request URI to target the right deployment:

  ```http
  POST https://<your-openai-resource>.openai.azure.com/openai/deployments/<deployment-name>/chat/completions?api-version=2024-05-01
  ```

ğŸ›‘ **Why others are wrong:**

- **API key** is for authentication, not routing.
- **Deployment endpoint**: All deployments share the **same base endpoint** (your OpenAI resource endpoint). The **deployment name** differentiates them.

---

ğŸ“˜ **Summary:**

| Purpose                   | Correct Option     |
| ------------------------- | ------------------ |
| Authentication            | âœ… API key         |
| Routing to specific model | âœ… Deployment name |

This setup ensures each app:

- Is authenticated securely.
- Connects only to the model intended for its workload.

---

## âŒ Question 47

![ex1_q47](images/ex1_q47.png)

---

### ğŸ§  Basic Conversion Rules

- âœ… **1 capacity = 1,000 TPM**
- âœ… **6 RPM = 1,000 TPM**

---

### â“ So...

**âœ… 1 capacity = 6 RPM!**

---

## âŒ Question 48

![alt text](image.png)

---

**âœ… The answer:**
**B. `"enabled": false`**

---

**ğŸ¤” Why This Is the Best Answer:**
To exclude a **custom brand** (like `"ITFirm"`) from being detected by **Azure Video Indexer's Brands model**, you must set:

```json
"enabled": false
```

This flags the brand for **exclusion** in the REST API configuration â€” meaning Azure Video Indexer will **skip it during automatic detection**.

According to Microsoftâ€™s documentation:

> Setting `"enabled": false` places the brand in the **Exclude list**.

---

**âŒ Why Other Options Are Wrong:**

- âŒ **A. `"enabled": true`**
  This puts the brand into the **Include list**, so it will **be detected**, not excluded.

- âŒ **C. `"state": "Excluded"`**
  `"state"` is **not a valid parameter** for brand exclusion in the API.

- âŒ **D. `"tags": ["CustomBrand"]`**
  Tags are metadata for categorization, not control switches for detection or exclusion.

- âŒ **E. `"useBuiltIn": false`**
  This disables detection from **Bingâ€™s built-in brand database**, but itâ€™s unrelated to excluding a **custom brand** like `"ITFirm"`.

---

ğŸ“˜ **Source Reference:**
[Customize the Brands Model â€“ Azure Video Indexer](https://learn.microsoft.com/en-us/azure/azure-video-indexer/customize-brands-model-how-to?tabs=customizeapi#prerequisites)

---

âœ… So remember:
To **exclude** a custom brand from detection in Azure Video Indexer â†’ use

```json
"enabled": false
```

---

## âŒ Question 53

![ex1_q53](images/ex1_q53.png)

---

**âœ… The correct answers:**

- âœ… **Migrate the Search service to a higher tier**
- âœ… **Add Replicas**

---

**ğŸ¤” Why These Are the Best Answers:**

### âœ… **1. Migrate to a Higher Tier**

- Higher tiers (like _Standard S2/S3_) offer:

  - **More throughput per unit**
  - **Higher limits** on requests per second
  - **Greater storage and compute capacity**

- This helps **absorb traffic spikes** from your travel appâ€™s 1M+ users.

### âœ… **2. Add Replicas**

- **Replicas = Query capacity**
- Each replica can handle **a set number of search requests in parallel**
- Adding more replicas helps **scale read operations**, reducing the chance of **throttling due to high query volume**

---

**âŒ Why Other Options Are Wrong:**

- âŒ **Add Indexes**

  - Indexes structure your data but do **not affect query throughput or throttling**
  - You can still get throttled even with multiple indexes if capacity is low

- âŒ **Enable customer-managed key (CMK) encryption**

  - CMK is for **data security**, not performance
  - It has **no effect** on request limits or query throttling

- âŒ **Deploy the app and private endpoint to a VNet**

  - This enhances **security** and **network isolation**, not performance
  - Throttling is due to **service capacity**, not network access

---

## âŒ Question 54

![ex1_q54](images/ex1_q54.png)

---

**âœ… The answer:**
**D. Add examples to the None intent**

---

**ğŸ¤” Why This Is the Best Answer:**
The **"None" intent** is used to catch utterances that **do not match any of your defined intents**. If you donâ€™t train it well, the model may **misclassify irrelevant input** as something meaningful â€” leading to **false positives**.

By adding **good examples of unrelated or out-of-scope phrases** to the **"None"** intent, you help the model:

- Recognize when the userâ€™s message **doesnâ€™t match any real intent**
- Avoid **incorrectly assigning** those queries to the "RequestSupport" or other active intents
- Reduce the chance of misclassification = **fewer false positives**

---

**âŒ Why Other Options Are Wrong:**

- âŒ **A. Add additional examples to the RequestSupport intent**

  - This improves generalization **within the intent**, but doesnâ€™t solve the problem of misclassifying **unrelated inputs** as â€œRequestSupport.â€ In fact, adding too many similar examples can even make the model **overfit**.

- âŒ **B. Add a machine learned entity**

  - Entities help **extract information** from within an utterance (e.g., â€œaccount numberâ€), but donâ€™t directly affect **intent classification** accuracy.

- âŒ **C. Enable active learning**

  - Active learning suggests **improvement data based on real usage**, but it wonâ€™t immediately reduce false positives unless you retrain with better-balanced data â€” especially for the â€œNoneâ€ intent.

---

ğŸ“˜ **Quick Tip for CLU / LUIS Training:**

| Strategy              | Effect                       |
| --------------------- | ---------------------------- |
| Add to RequestSupport | Better generalization        |
| Add to None           | â— Fewer false positives âœ…  |
| Add entities          | More precise data extraction |
| Use active learning   | Iterative refinement         |

---

âœ… **Final Advice:**
For **intent precision**, always include a **strong â€œNoneâ€ intent** with 50â€“100+ unrelated phrases. Itâ€™s **essential** in virtual assistant design to avoid confusion.

---

## âŒ Question 56
