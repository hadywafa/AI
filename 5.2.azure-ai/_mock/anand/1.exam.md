# 🏁 Exam 1

## ❌ Question 6

![ex1_q6](images/ex1_q6.png)

---

**✅ The answer:**
`1. https://centralus.api.cognitive.microsoft.com`
`2. /text/analytics/v3.1/languages`

---

**🤔 Why This Is the Best Answer:**

- `https://centralus.api.cognitive.microsoft.com`: This is a valid **base endpoint** for Azure Cognitive Services in the **Central US region**, commonly used for services like Text Analytics (Language Detection).
- `/text/analytics/v3.1/languages`: This is the correct **API path** for detecting the language of a given input using the **Azure AI Language** service. It helps you determine whether a customer query is in English or Spanish so you can route it accordingly.

---

**❌ Why Other Options Are Wrong:**

- `https://eu.api.cognitive.microsofttranslator.com`: While valid for Translator, it doesn't relate to **language detection**; it's just a **regional base URL**.
- `/translator/text/v3.0/translate?to=es` and `/translate?to=en`: These are **translation endpoints**, not for detecting language. Translation is **not required** in this use case since the goal is routing based on **original message language**, not converting it.
- `https://portal.azure.com`: This is just the **web portal**, not an API endpoint. It cannot be used in code to detect or translate language.

---

## ❌ Question 12

![ex1_q12](images/ex1_q12.png)

---

**✅ The answer:**

- **A. Use the Language Studio's performance dashboard to view metrics and evaluation reports.**
- **C. Retrieve the evaluation report from the authoring API provided by the language service.**

---

**🤔 Why This Is the Best Answer:**
These two methods directly support **pre-deployment evaluation**:

- ✅ **A. Language Studio’s performance dashboard** gives you an interactive view of how your model performs on test data — showing **metrics like precision, recall, F1 score**, confusion matrices, and misclassified utterances.

- ✅ **C. Authoring API for evaluation summary** allows you to **retrieve detailed evaluation reports** via REST. You can use this in automation or integrate it into your CI/CD pipeline.

Example API:

```http
GET https://{endpoint}/language/authoring/analyze-conversations/projects/{projectName}/models/{trainedModelLabel}/evaluation/summary-result?api-version=2023-04-01
```

---

**❌ Why Other Options Are Wrong:**

- ❌ **B. Enable feedback collection...**
  Feedback collection is part of a **post-deployment** improvement loop. It relies on **live user interaction logs** and does **not help in pre-deployment** evaluation.

- ❌ **D. Azure Monitor**
  Azure Monitor is used for **infrastructure and service health**, not for evaluating the **model's linguistic understanding**. It tracks **performance metrics like latency and availability**, not NLP accuracy.

---

📘 **Key Concept (AI-102):**

> **Pre-deployment evaluation** should be done using test datasets, evaluation reports, and built-in performance dashboards — **not production feedback or monitoring tools**.

---

## ❌ Question 15

![ex1_q15](images/ex1_q15.png)

---

**✅ The answer:**

- **A. to=el**
- **C. toScript=Latn**
- **E. textType=html**

---

**🤔 Why This Is the Best Answer:**

- ✅ **A. `to=el`** – This sets the **target language to Greek**, which is the main translation objective. `"el"` is the correct ISO language code for Greek.

- ✅ **C. `toScript=Latn`** – This enables **Roman (Latin script) transliteration** of the translated Greek text. Greek normally uses the Greek script, so `toScript=Latn` provides a phonetic version readable by non-Greek users.

- ✅ **E. `textType=html`** – This indicates the input is **HTML content**, so the API treats tags appropriately and doesn’t translate elements like `<div>` or `<strong>`. This is important for product descriptions that may contain formatting.

---

**❌ Why Other Options Are Wrong:**

- ❌ **B. `from=fr`** – This is only useful if the source text is explicitly in French. Since source detection is automatic by default and the question doesn’t mention French, this is unnecessary.

- ❌ **D. `textType=xml`** – This is not correct for standard product description text. Unless you're passing actual XML documents, `textType=html` is more appropriate.

- ❌ **F. `toScript=Cyrl`** – `Cyrl` is Cyrillic script (used in Russian, Serbian, etc.), which is **not appropriate for Greek**. Greek uses its own script and transliterates to **Latin (Latn)**, not Cyrillic.

---

## ❌ Question 16

![ex1_q16](images/ex1_q16.png)

---

**✅ The answer:**
**(A) 1, 4 — `StorageConnectionString`, `Objects`**

---

**🤔 Why This Is the Best Answer:**
To build a functional **Knowledge Store** in Azure AI Search that supports **enriched data like Sentiment Analysis**, you need to define:

- ✅ **`StorageConnectionString`** – This is **mandatory** to connect your Knowledge Store to an **Azure Storage account** where enriched data (tables, objects) will be saved.
- ✅ **`Objects`** – This defines **projection formats** such as enriched JSON documents. Objects are used when you want to store complete enriched documents (e.g., customer review + sentiment + key phrases) for flexible retrieval or further processing.

These two together allow:

- Saving enriched results in a structured but flexible way.
- Retrieving entire enriched documents (great for dashboards, analytics, or full reviews).

---

**❌ Why Other Options Are Wrong:**

- ❌ **`Tables`** – While valid, it’s optional and only needed when you want **normalized/tabular projections** (like splitting key phrases or entities into separate rows). It’s not the best **default** if you want flexible, full-document access.

- ❌ **`ContainerName`** – This is **not a valid field** in the `knowledgeStore` definition block. Azure uses the storage connection string to resolve the correct container or blob context.

- ❌ **C) 1, 2 (`StorageConnectionString`, `Tables`)** – Close, but not ideal for **flexible access**. Tables are great for relational processing, not for retrieving the entire enriched review.

- ❌ **B) 3, 2 (`ContainerName`, `Tables`)** – Incorrect. `ContainerName` is not used in Knowledge Store definitions. Plus, it omits the required `StorageConnectionString`.

---

📦 **Correct Knowledge Store JSON (simplified):**

```json
"knowledgeStore": {
  "storageConnectionString": "<your-azure-blob-connection-string>",
  "projections": [
    {
      "objects": [
        {
          "source": "/document",
          "storageContainer": "enriched-reviews"
        }
      ]
    }
  ]
}
```

---

## ✅ Question 20

![ex1_q20](images/ex1_q20.png)

**✅ The answer:**
**C) Change Domains to General (compact) → Retrain the model → Export the model**

---

**🤔 Why This Is the Best Answer:**
To export a model for **offline or edge deployment**, Azure Custom Vision requires that you use a **Compact domain** (like _General (compact)_), which supports model export in ONNX, TensorFlow, or other embedded formats.

Here’s why each step is necessary:

1. ✅ **Change Domains to General (compact):**

   - Only **Compact domains** support **exporting models**. The standard "General" domain does **not support export**.

2. ✅ **Retrain the model:**

   - After changing the domain, you **must retrain** the model — Custom Vision models are tied to a specific domain during training.

3. ✅ **Export the model:**

   - Once retrained with a compact domain, the **export button becomes available**, allowing you to download the model in formats like ONNX or TensorFlow Lite for use on devices without internet access.

---

**❌ Why Other Options Are Wrong:**

- ❌ **A. Optimize model for edge deployment** – There’s no separate "optimize" step in Custom Vision. The optimization happens automatically when you **train using a compact domain**.

- ❌ **B. Retrain the model** (first) – Retraining before switching to a compact domain is **pointless**, because only compact domains allow exports. You’d have to retrain again anyway after switching.

- ❌ **D. Change the classification type** – This isn’t related to exportability. It determines **how labels are assigned** (multiclass vs. multilabel), not deployment compatibility.

- ❌ **E. Create a new classification model** – You can reuse the existing project by just changing its domain and retraining. Creating a new model isn’t necessary unless you want to start from scratch.

---

🛠️ **Pro Tip:**
For exporting a model to work on disconnected or embedded environments like Raspberry Pi, microcontrollers, or industrial devices, **always use Compact domains** in Custom Vision.

---

📘 **Final Workflow:**
👉 **Change to General (compact)** → 🔁 **Retrain the model** → ⬇️ **Export the model for offline use**.

---

## ❌ Question 29

![ex1_q29](images/ex1_q29.png)

---

![ex1_q29_answer](images/ex1_q29_answer.png)

Why other options are not correct:

No logging options are mentioned in the docker run command. For ex:

![ex1_q29_answer_explain](images/ex1_q29_answer_explain.png)

---

## ❌ Question 44

![ex1_q44](images/ex1_q44.png)

---

**✅ The answer:**
**Jailbreak risk detection**

---

**🤔 Why This Is the Best Answer:**

- **Jailbreak risk detection** is a **specialized safety feature** in Azure OpenAI that helps detect **attempts to bypass or manipulate content filters**.
- It identifies **malicious prompts** designed to get the model to output harmful, unsafe, or restricted content (e.g., asking the model to "pretend" or "ignore safety rules").
- This is **critical for protecting conversational AI apps** that rely on OpenAI models, especially before or alongside traditional moderation techniques.

---

**❌ Why Other Options Are Wrong:**

- ❌ **Moderate text content** – This is part of **Azure AI Content Safety**, which detects **harmful, sexual, hate, or violent language**, but it **doesn't detect jailbreak attempts** that try to trick the model without using explicit harmful content.

- ❌ **Protected material detection** – This is for detecting **copyrighted content**, such as music, code, or text fragments—not applicable to **malicious intent** in prompts.

- ❌ **Groundedness detection** – This checks if an LLM’s answer is **factually supported** by source documents (used in RAG apps). It doesn't identify **user prompt manipulation or jailbreaks**.

---

📘 **Pro Tip for AI-102 & Real Apps:**
Use **Jailbreak risk detection** for:

- Guarding against **prompt injection**
- Blocking **model manipulation** attempts
- Working alongside **content moderation** and **user input filters** for robust safety

🔐 Combine it with **Content Safety APIs** for a **layered defense strategy** in production apps.

---

## ❌ Question 45

![ex1_q45_1](images/ex1_q45_1.png)
![ex1_q45_2](images/ex1_q45_2.png)

---

**✅ The answer:**
**None of the above**

---

**🤔 Why This Is the Best Answer:**

Let’s break down the statements and explain why **each one is false** based on the response JSON and model settings:

---

🔍 Statement: _"The OpenAI model used is text-embedding-ada-002"_

**❌ False!**

- The model clearly specified in the response is:
  `"model": "gpt-3.5-turbo-0301"`
- `text-embedding-ada-002` is used **only for embeddings**, not for chat or completions.

---

🔍 Statement: _"The prompt_tokens value will be included in the calculation of the Max response tokens value."_

**❌ False!**

- `max_tokens` (or max response tokens) applies **only to the output tokens** (completion).
- The prompt (input) token count is not included in this cap.
  👉 In this case:

  - `prompt_tokens = 37`
  - `completion_tokens = 86`
  - `max_response_tokens = 100` → the model was allowed to generate up to 100 **output** tokens.

---

🔍 Statement: _"The subscription will be charged 86 tokens for the execution of the session."_

**❌ False!**

- Billing in Azure OpenAI is based on **total tokens** used:

  - `prompt + completion = 37 + 86 = 123 tokens`

- So you are charged for **123 tokens**, not just the output.

---

🔍 Statement: _"The text completion was cut off due to exceeding the Max response tokens limit."_

**❌ False!**

- The setting allows for up to **100 response tokens**, and the model only used **86**, well within the limit.
- Also, the `"finish_reason": "stop"` confirms the model ended naturally.
  If it had hit the max token cap, it would’ve said: `"finish_reason": "length"`

---

**✅ Summary:**
None of the options are true based on the model used, token accounting, billing logic, or how completions are handled.

---

📘 **Pro Tip (AI-102 or devs):**
To avoid surprises in billing and truncation:

- Always monitor both `prompt_tokens` and `completion_tokens`
- Use `"finish_reason"` to programmatically handle truncated responses.

---

## ✅ Question 46

![ex1_q46](images/ex1_q46.png)

---

**✅ The answer:**
**B) An API key, A deployment name**

---

**🤔 Why This Is the Best Answer:**

To allow three different apps to access **three separate deployments** of the GPT-3.5 model under the same Azure OpenAI resource, the setup should be:

---

### 🔐 **1. Provide access to AIDev by using: An API key**

- ✅ Azure OpenAI uses **API keys** for authenticating requests made via REST API.
- Each request must include this key in the `Ocp-Apim-Subscription-Key` header.
- This enables easy control and avoids needing Azure AD integration for typical use.

🛑 **Why others are wrong:**

- **Bearer token / AD token:** These are used for **Azure AD-based** access, not commonly required or configured for OpenAI resources unless using RBAC and managed identities, which isn’t the default method for REST-based access.

---

### 🔗 **2. Connect to the deployment by using: A deployment name**

- ✅ Even though all three deployments share the same **resource endpoint**, each one has a unique **deployment name** (e.g., `gpt-chat-support`, `gpt-recommendation`, `gpt-content-gen`).
- You must include this **deployment name** in the request URI to target the right deployment:

  ```http
  POST https://<your-openai-resource>.openai.azure.com/openai/deployments/<deployment-name>/chat/completions?api-version=2024-05-01
  ```

🛑 **Why others are wrong:**

- **API key** is for authentication, not routing.
- **Deployment endpoint**: All deployments share the **same base endpoint** (your OpenAI resource endpoint). The **deployment name** differentiates them.

---

📘 **Summary:**

| Purpose                   | Correct Option     |
| ------------------------- | ------------------ |
| Authentication            | ✅ API key         |
| Routing to specific model | ✅ Deployment name |

This setup ensures each app:

- Is authenticated securely.
- Connects only to the model intended for its workload.

---

## ❌ Question 47

![ex1_q47](images/ex1_q47.png)

---

### 🧠 Basic Conversion Rules

- ✅ **1 capacity = 1,000 TPM**
- ✅ **6 RPM = 1,000 TPM**

---

### ❓ So...

**✅ 1 capacity = 6 RPM!**

---

## ❌ Question 48

![alt text](image.png)

---

**✅ The answer:**
**B. `"enabled": false`**

---

**🤔 Why This Is the Best Answer:**
To exclude a **custom brand** (like `"ITFirm"`) from being detected by **Azure Video Indexer's Brands model**, you must set:

```json
"enabled": false
```

This flags the brand for **exclusion** in the REST API configuration — meaning Azure Video Indexer will **skip it during automatic detection**.

According to Microsoft’s documentation:

> Setting `"enabled": false` places the brand in the **Exclude list**.

---

**❌ Why Other Options Are Wrong:**

- ❌ **A. `"enabled": true`**
  This puts the brand into the **Include list**, so it will **be detected**, not excluded.

- ❌ **C. `"state": "Excluded"`**
  `"state"` is **not a valid parameter** for brand exclusion in the API.

- ❌ **D. `"tags": ["CustomBrand"]`**
  Tags are metadata for categorization, not control switches for detection or exclusion.

- ❌ **E. `"useBuiltIn": false`**
  This disables detection from **Bing’s built-in brand database**, but it’s unrelated to excluding a **custom brand** like `"ITFirm"`.

---

📘 **Source Reference:**
[Customize the Brands Model – Azure Video Indexer](https://learn.microsoft.com/en-us/azure/azure-video-indexer/customize-brands-model-how-to?tabs=customizeapi#prerequisites)

---

✅ So remember:
To **exclude** a custom brand from detection in Azure Video Indexer → use

```json
"enabled": false
```

---

## ❌ Question 53

![ex1_q53](images/ex1_q53.png)

---

**✅ The correct answers:**

- ✅ **Migrate the Search service to a higher tier**
- ✅ **Add Replicas**

---

**🤔 Why These Are the Best Answers:**

### ✅ **1. Migrate to a Higher Tier**

- Higher tiers (like _Standard S2/S3_) offer:

  - **More throughput per unit**
  - **Higher limits** on requests per second
  - **Greater storage and compute capacity**

- This helps **absorb traffic spikes** from your travel app’s 1M+ users.

### ✅ **2. Add Replicas**

- **Replicas = Query capacity**
- Each replica can handle **a set number of search requests in parallel**
- Adding more replicas helps **scale read operations**, reducing the chance of **throttling due to high query volume**

---

**❌ Why Other Options Are Wrong:**

- ❌ **Add Indexes**

  - Indexes structure your data but do **not affect query throughput or throttling**
  - You can still get throttled even with multiple indexes if capacity is low

- ❌ **Enable customer-managed key (CMK) encryption**

  - CMK is for **data security**, not performance
  - It has **no effect** on request limits or query throttling

- ❌ **Deploy the app and private endpoint to a VNet**

  - This enhances **security** and **network isolation**, not performance
  - Throttling is due to **service capacity**, not network access

---

## ❌ Question 54

![ex1_q54](images/ex1_q54.png)

---

**✅ The answer:**
**D. Add examples to the None intent**

---

**🤔 Why This Is the Best Answer:**
The **"None" intent** is used to catch utterances that **do not match any of your defined intents**. If you don’t train it well, the model may **misclassify irrelevant input** as something meaningful — leading to **false positives**.

By adding **good examples of unrelated or out-of-scope phrases** to the **"None"** intent, you help the model:

- Recognize when the user’s message **doesn’t match any real intent**
- Avoid **incorrectly assigning** those queries to the "RequestSupport" or other active intents
- Reduce the chance of misclassification = **fewer false positives**

---

**❌ Why Other Options Are Wrong:**

- ❌ **A. Add additional examples to the RequestSupport intent**

  - This improves generalization **within the intent**, but doesn’t solve the problem of misclassifying **unrelated inputs** as “RequestSupport.” In fact, adding too many similar examples can even make the model **overfit**.

- ❌ **B. Add a machine learned entity**

  - Entities help **extract information** from within an utterance (e.g., “account number”), but don’t directly affect **intent classification** accuracy.

- ❌ **C. Enable active learning**

  - Active learning suggests **improvement data based on real usage**, but it won’t immediately reduce false positives unless you retrain with better-balanced data — especially for the “None” intent.

---

📘 **Quick Tip for CLU / LUIS Training:**

| Strategy              | Effect                       |
| --------------------- | ---------------------------- |
| Add to RequestSupport | Better generalization        |
| Add to None           | ❗ Fewer false positives ✅  |
| Add entities          | More precise data extraction |
| Use active learning   | Iterative refinement         |

---

✅ **Final Advice:**
For **intent precision**, always include a **strong “None” intent** with 50–100+ unrelated phrases. It’s **essential** in virtual assistant design to avoid confusion.

---

## ❌ Question 56

![ex1_q56](images/ex1_q56.png)

---

**✅ The correct answers:**

- ✅ **B. Configure a custom subdomain**
- ✅ **E. Create a Conditional Access policy**

---

**🤔 Why These Are the Best Answers:**

### ✅ **B. Configure a Custom Subdomain**

- Azure AI services **must have a custom subdomain** (e.g., `yourapp.cognitiveservices.azure.com`) to support **Microsoft Entra ID (formerly Azure AD)** authentication.
- Without it, **token-based authentication won't work**.
- This is a **mandatory prerequisite** for Entra ID access tokens.

> 🔗 Source: [Azure AI Service Authentication with Entra ID](https://learn.microsoft.com/en-us/azure/ai-services/authentication#authenticate-with-microsoft-entra-id)

---

### ✅ **E. Create a Conditional Access Policy**

- **Optional but recommended** for **enhanced security**.
- Allows you to **control access** (who, from where, on what device) to the Azure AI resource **via Entra ID**.
- Useful in **regulatory and enterprise security** scenarios.

---

**❌ Why Other Options Are Incorrect:**

- ❌ **A. Request an X.509 certificate**

  - Not required for Entra ID token auth.
  - X.509 certs are for **certificate-based auth**, not **OAuth2/Entra ID**-based flows.

- ❌ **C. Enable a virtual network service endpoint**

  - Related to **network security**, not **identity or authentication**.
  - Has **no impact** on enabling Entra ID auth.

- ❌ **D. Create a private endpoint**

  - Like service endpoints, this is for **restricting network access**, not for configuring or enabling identity-based access via Entra ID.

---

## ✅ Question 57

![ex1_q57](images/ex1_q57.png)

---

**✅ The correct sequence of actions:**
**A) 3 → 6 → 4**
(1️⃣ Select v1.1 → 2️⃣ Export for containers (GZIP) → 3️⃣ Run and mount model file)

---

### 🧩 Step-by-Step Breakdown

#### ✅ **Step 1: Select v1.1 of MyApp**

- **Why v1.1?**

  - **v1.2** is the newest but **not trained** → ❌ Not deployable
  - **v1.0** is older and already published, but v1.1 is newer **and trained** → ✅ Best valid choice

- ✅ **You must select the latest _trained_ version**, even if unpublished

---

#### ✅ **Step 2: Export using “Export for containers (GZIP)”**

- Containers **require the model in GZIP format** to load it locally
- Do **not** use “Export as JSON” — that’s for non-container use

---

#### ✅ **Step 3: Run the container and mount the model file**

- Mount the GZIP file into the container using a volume
- The container loads the model at runtime from the mounted path

---

### ❌ Why Other Choices Are Incorrect:

- **v1.2** is untrained → can’t be used
- **v1.0** is older than v1.1 → doesn’t meet the “latest deployable” requirement
- **Environment variable** for version is meaningless if the model isn't published or mounted
- **JSON export** isn’t compatible with containers

---

### ✅ Final Sequence Recap:

| Step | Action                                     | Reason                            |
| ---- | ------------------------------------------ | --------------------------------- |
| 1    | **Select v1.1** of MyApp                   | Latest trained version ✅         |
| 2    | **Export for containers (GZIP)**           | Required format for containers ✅ |
| 3    | **Run container and mount the model file** | Loads model at runtime ✅         |

Let me know if you'd like the `docker run` command syntax for the container setup!

## ❌ Question 58

![ex1_q58](images/ex1_q58.png)

---

**✅ The correct answer:**
**D. Apply virtual network rules to the Language service instance**

---

**🤔 Why This Is the Best Answer:**

- **Virtual network (VNet) rules** allow you to **restrict access to your Azure AI Language service** so that **only resources within a specific VNet** can access it.
- This effectively **isolates your Language service from the public internet**, which is **exactly what’s needed** for processing sensitive customer data and meeting **compliance requirements**.
- VNet rules are **simple to configure** and **don’t require extensive setup** (unlike gateways or complex routing).

✅ This is the **recommended and built-in way** to secure Azure Cognitive Services (like Language) using **network access controls**.

> 📘 See: [Secure Cognitive Services with virtual networks](https://learn.microsoft.com/en-us/azure/cognitive-services/cognitive-services-virtual-networks)

---

**❌ Why Other Options Are Wrong:**

- ❌ **A. Configure IPsec rules**

  - IPsec is **not supported** for Azure AI services.
  - It's used for VPNs and is **not manageable at the service level**.

- ❌ **B. Use Azure Application Gateway**

  - App Gateway is used for **web traffic routing** and **load balancing**, not for **securing backend Azure AI services**.

- ❌ **C. Set up a virtual network gateway**

  - This enables VPN access but does **not filter access** to Azure services.
  - It's **overkill** and doesn’t meet the specific requirement.

---

## ❌ Question 59

![ex1_q59](images/ex1_q59.png)

---

## ❌ `Question 60`

![ex1_q60](images/ex1_q60.png)

---

**✅ The correct answer:**
**B. Deploy the Language Service in a container on your on-premises infrastructure**

---

**🤔 Why This Is the Best Answer:**

- Azure AI Language service supports **containerized deployment**, which allows you to run the **full Language service (including sentiment analysis and LUIS models)** on **your own infrastructure** — without requiring public cloud access.
- This approach:

  - ✅ **Meets strict security/compliance needs**
  - ✅ **Keeps all data on-premises**
  - ✅ **Avoids using any public Azure endpoints**

- Containers are downloadable from **Microsoft Container Registry (MCR)** and support features like **authentication via local environment variables**, **offline inference**, and **full network isolation**.

> 📘 Reference: [Azure AI containers – Language Service](https://learn.microsoft.com/en-us/azure/cognitive-services/cognitive-services-container-support)

---

**❌ Why Other Options Are Wrong:**

- ❌ **A. Configure Azure VM and restrict IPs**

  - Still uses **Azure-hosted infrastructure**, violating the "no public cloud" requirement.

- ❌ **C. Export model as ONNX**

  - The Language Service does **not support exporting its models** (like sentiment or key phrase extraction) as ONNX — only some vision services support ONNX exports.

- ❌ **D. Use private endpoint in Azure**

  - A private endpoint **still requires the service to be hosted in Azure**, which again **violates the no-public-cloud policy**.

---

## ❌ Question 61

![ex1_q61](images/ex1_q61.png)

---

**✅ Correct answer:**
**C. Set the Temperature parameter for the model to 0**

---

### 🤔 Why This Is the Best Answer:

- **Temperature** controls the **randomness** of the model's responses:

  - **0** = very **deterministic** (always chooses the most likely token)
  - **Higher values (e.g., 0.7–1)** = more **creative** or **diverse** responses

- For scenarios like **product support, troubleshooting, or factual Q\&A**, you want the model to be:

  - **Predictable**
  - **Consistent**
  - **Accurate**

- Therefore, **setting temperature to 0** makes the model return **the most probable and stable answer every time**, which is ideal for a **support chatbot**.

---

### ❌ Why Other Options Are Wrong:

- **A. Configure the model to include data from the retail company's knowledge base**

  - Azure OpenAI **cannot directly access external databases or knowledge bases**.
  - To use a knowledge base, you'd need **RAG (Retrieval-Augmented Generation)** via **Azure Cognitive Search + Embeddings**, not a setting change.
  - Also, that alone doesn’t guarantee **accuracy** — only **relevance**.

- **B. Set the Top P parameter for the model to 0**

  - Top P = nucleus sampling; controls **how much of the probability distribution to consider**.
  - It **must be between 0 and 1** — setting it to **0 is invalid** and will likely cause the API to reject the request.

- **D. Modify the system message to specify that answers must be accurate**

  - This can **influence** behavior but does **not guarantee** accuracy.
  - Without proper **temperature control**, the model may still return variable or imaginative responses.

---

## ❌ Question 62

![ex1_q62](images/ex1_q62.png)

---

## ❌ Question 65

![ex1_q65](images/ex1_q65.png)

---

## ❌ Question 70

![ex1_q70](images/ex1_q70.png)

---

**✅ Correct answers:**
**B. Changing "What is an LLM?" to "What is an LLM in the context of AI models?" will produce a more accurate response**
**D. Changing "You are a helpful assistant." to "You must answer only within the context of AI language models." will produce a more relevant response**

---

### 🤔 Why These Are the Best Answers:

---

### ✅ **B. Adding context improves accuracy**

- GPT models perform **better with more specific prompts**.
- `"What is an LLM?"` is general and may include legal or other meanings.
- Adding `"in the context of AI models"` helps the model **narrow its focus**, improving **relevance and precision**.

---

### ✅ **D. Refined system messages steer the model**

- The system message is used to **set the assistant's role and response style**.
- Changing it to:
  `"You must answer only within the context of AI language models"`
  **tightens the boundaries**, avoiding off-topic or overly generic replies.
- This makes the assistant **more context-aware**, especially helpful in **domain-specific apps** like your glossary.

---

### ❌ Why the Other Options Are Incorrect:

---

### ❌ **A. “The code guarantees that responses will be sourced from the latest, up-to-date AI research.”**

- GPT models are **pre-trained**, not connected to live data.
- They do **not fetch real-time updates**, so no guarantee of being "up-to-date."
- Responses are based on the **model's training cutoff**, e.g., Sep 2021 or April 2023 depending on model.

---

### ❌ **C. “The code will only retrieve definitions from trusted AI sources.”**

- GPT doesn't “retrieve” at all. It **generates** answers from its internal language model.
- There's no verification of **trusted sources** or citation from real documents — unless you integrate **RAG** (Retrieval-Augmented Generation).

---

## ❌ Question 72

![ex1_q72](images/ex1_q72.png)

---

| Feature                     | **F0 (Free Tier)**                | **S0 (Standard Tier)**               |
| --------------------------- | --------------------------------- | ------------------------------------ |
| **File formats supported**  | PDF, JPEG, PNG, TIFF              | PDF, JPEG, PNG, TIFF                 |
| **Max file size**           | **4 MB**                          | **500 MB**                           |
| **Max pages per file**      | **2 pages**                       | **Up to 2,000 pages**                |
| **Image dimensions**        | 50 x 50 to 10,000 x 10,000 pixels | 50 x 50 to 10,000 x 10,000 pixels    |
| **PDF password protection** | Must be **unlocked**              | Must be **unlocked**                 |
| **Request rate limits**     | Lower (for testing/dev only)      | Higher (for production scale)        |
| **Billing**                 | Free (limited usage)              | Pay-as-you-go pricing                |
| **Max concurrency**         | Very limited                      | Supports multiple parallel requests  |
| **Use case**                | Prototyping / personal testing    | Production / commercial applications |

## ❌ `Question 76`

![ex1_q76](images/ex1_q76.png)

---

**✅ The correct answers:**
**D. Configure virtual network settings in TextAnalysisService**
**E. Create a new private endpoint for the TextAnalysisService in CompanyVNet**

---

### 🤔 Why These Are the Best Answers:

---

### ✅ **D. Configure virtual network settings in TextAnalysisService**

- You must **enable network isolation** on the **TextAnalysisService** itself by configuring its **VNet integration settings**.
- This ensures that **only traffic from your CompanyVNet is allowed**, and **public access is denied**.
- This is the **core control** to restrict the service to VNet-only traffic.

---

### ✅ **E. Create a private endpoint for TextAnalysisService in CompanyVNet**

- A **private endpoint** assigns the service a **private IP address within the VNet**, ensuring that:

  - Communication stays **within the Azure backbone** (no internet hops)
  - External (public internet) traffic **cannot access** the service

- This is the **most secure and recommended** method for isolating Azure services.

---

### ❌ Why Other Options Are Incorrect:

- ❌ **A. Enable a service endpoint for the TextAnalysisService in CompanyVNet**

  - **Service endpoints** allow resources **in the VNet to reach public Azure services**, but the service itself is still **publicly accessible** unless additional firewall rules are set.
  - This does **not block internet access** and is **less secure** than private endpoints.

- ❌ **B. Configure RBAC in TextAnalysisService**

  - **RBAC** controls **who can manage** or configure the service, not **what network traffic can reach it**.
  - Doesn’t help with **network isolation** or internet access restriction.

- ❌ **C. Modify the network settings for CompanyVNet**

  - Adjusting the VNet alone won’t affect access to the Cognitive Service unless the **service itself is configured** to accept only private traffic.
  - Network changes must be paired with service-level configurations.

---

## ❌ Question 90

![ex1_q90](images/ex1_q90.png)

---

### ❓**The Scenario Recap:**

- A news organization publishes **frequent updates**.
- They need **summarization** + **Q\&A**.
- The **knowledge base updates constantly**.
- They want **low cost**, **no retraining**, **quick setup**.

---

### ✅ Correct Answer:

**A. RAG with Azure OpenAI on your data!**

> _Because it uses a **stateless API** and integrates with **Azure AI Search** to incorporate new information **without retraining**._

---

### ❌ Your Answer (C):

**"RAG with Azure OpenAI, but only if they unselect the option that encourages the model to use their data…"**
This is **incorrect** for two reasons:

---

### 🧠 1. **Misunderstanding RAG’s purpose**

RAG (Retrieval-Augmented Generation) is **built to use your external data** (like news articles, PDFs, etc.) at **query time**.
You **don’t want to disable** the option for the model to use your data — **that’s the whole point** of RAG.

Your selected answer implies:

> “We should tell the model to use only its own pretraining.”

But the need here is **live use of updated articles**, so relying on **static pretraining** would **not meet the requirement**.

---

## ❌ Question 91

![ex1_q91](images/ex1_q91.png)
