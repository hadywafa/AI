# ğŸ§ âš™ï¸ Building a Local Agent That Executes Functions in Your Laptop Using Azure OpenAI Agents

Welcome to the next exciting installment of **LLM Agents in Actionâ„¢**. Last time, we asked the agent to create a chart from CSV using Python **on Azure's remote compute** (fancy!). This time, we are flipping the table:

> â€œLetâ€™s keep it **local**. Laptop-local. Like ramen and sweatpants.â€

In this demo, youâ€™ll build an Azure AI Agent that talks to **LLMs in the cloud** but runs **your own Python functions right on your laptop**, in your Python runtime.

Sounds small?

ğŸ§  Think again â€” this is the very foundation of integrating **LLMs with local environments, devices, or apps**. Itâ€™s how ChatGPT could one day launch your game, shut down your PC, or even schedule your laundry robot.

---

## ğŸ¯ What This Demo Shows

Instead of asking the model to generate code or produce results with its inbuilt tools like a code interpreter, weâ€™re **connecting it to a real Python function**:

```python
fetch_weather("New York")
```

This function is:

- Locally defined ğŸ§‘â€ğŸ’»
- Locally executed âš™ï¸
- Dynamically called by the LLM ğŸ¤–
- Handled via an **Agent** in Azure AI Foundry ğŸ› ï¸

In short: we create a bot that _thinks_ in the cloud but _acts_ on your machine.

---

## ğŸ“‹ Use Case Example: Weather Bot ğŸŒ¤ï¸

Letâ€™s say you want a bot that responds like:

> "The weather in New York is Sunny, 25Â°C."

Instead of the model hallucinating the weather, we **delegate this job** to a real local function, which returns reliable (mocked) data.

This is how agents create magic â€” pairing **smart LLMs** with **actual capabilities**.

---

## ğŸ—ï¸ Full Workflow Overview

```mermaid
graph TD
  A[User Prompt: 'What's the weather?'] --> B[Azure Agent]
  B --> C["LLM (GPT-4o-mini)"]
  C -->|Wants weather| D[Local Function Tool]
  D --> E["fetch_weather('New York')"]
  E --> F[Returns 'Sunny, 25Â°C']
  F --> C
  C --> G[Final Answer: 'The weather is Sunny, 25Â°C.']
```

---

## ğŸ§± Full Breakdown of the Demo

### ğŸ” 1. Connect to Azure AI Foundry Project

We start by logging into Azure and retrieving your Foundry Project connection string:

```python
project_client = AIProjectClient.from_connection_string(
    credential=DefaultAzureCredential(),
    conn_str="xxxxxxxxxxxxxxxx"
)
```

This lets your code authenticate and create agents, threads, and messages in the Foundry.

---

### ğŸ§° 2. Define Local Function (Your Tool)

This is your **tool** that the agent will use:

```python
def fetch_weather(location: str) -> str:
    mock_weather_data = {
        "New York": "Sunny, 25Â°C",
        "London": "Cloudy, 18Â°C",
        "Tokyo": "Rainy, 22Â°C"
    }
    return json.dumps({"weather": mock_weather_data.get(location, "Not available")})
```

You can plug in **real APIs** (e.g., OpenWeatherMap) if you want, but here itâ€™s mocked for simplicity.

Wrap it inside a Tool:

```python
from azure.ai.projects.models import FunctionTool
functions = FunctionTool(user_functions)
```

And group the tools using:

```python
from azure.ai.projects.models import ToolSet
toolset = ToolSet()
toolset.add(functions)
```

---

### ğŸ¤– 3. Create the Agent (LLM + Local Power)

Now, create an **agent** that combines:

- The LLM (GPT-4o-mini)
- The toolset (with your local functions)

```python
agent = project_client.agents.create_agent(
    model="gpt-4o-mini",
    name="weather-agent",
    instructions="You are a helpful weather bot. Use the provided function to answer weather questions.",
    toolset=toolset
)
```

ğŸ”¥ Boom! You now have an LLM that can reach into your Python code and call real functions.

---

### ğŸ§µ 4. Create the Thread and Message

Send a prompt to the agent like:

```python
message = project_client.agents.create_message(
    thread_id=thread.id,
    role="user",
    content="Hello, send an email with the datetime and weather information in New York?",
)
```

> Behind the scenes, the LLM will realize it needs weather data, and call your `fetch_weather()` function locally.

---

### ğŸƒâ€â™‚ï¸ 5. Run the Agent Workflow

Trigger the agent:

```python
run = project_client.agents.create_and_process_run(thread_id=thread.id, agent_id=agent.id)
```

Youâ€™ll see in your debugger (with a breakpoint!) that the function gets hit:

```bash
>>> BREAKPOINT HIT: fetch_weather()
```

LLM called your Python function â€” without any hardcoded `if prompt == â€œweatherâ€:` logic. It inferred intent from the user prompt!

---

### ğŸ“¤ 6. View the Final Response

The message from the agent:

```json
"The weather in New York is sunny with a temperature of 25Â°C."
```

> Auto-magically created, formatted, and returned. No templates. No APIs. Pure agent-driven power.

---

## ğŸ’¡ Agent Local Function Use Cases

This pattern is insanely useful:

| ğŸ§© Use Case        | What Agent Can Call                 |
| ------------------ | ----------------------------------- |
| Smart Home Control | `turn_on_lights()`, `lock_doors()`  |
| System Monitoring  | `check_cpu_usage()`, `disk_space()` |
| Email Automation   | `send_email()`, `generate_report()` |
| File Automation    | `rename_files()`, `zip_logs()`      |
| App Scripting      | `launch_game()`, `shutdown_pc()`    |

Itâ€™s the **foundation** for personal assistants like Jarvis. ğŸ’¡

---

## ğŸ§¨ Debugging: The Library Trap

You might have done everything right... but suddenly:

> âŒ Boom. Error. Nothing works. Agent fails silently or errors out.

Hereâ€™s what happened in our case:

### ğŸª² Root Cause:

The latest version of the `azure-ai-projects` Python package was **broken** (`b9`). Our code only worked on `b8`.

### ğŸ› ï¸ Fix:

```bash
pip uninstall azure-ai-projects
pip install azure-ai-projects==1.0.0b8
```

> ğŸ‘¿ Welcome to the "DLL Hell" of AI â€” now with more tokens.

LLMs couldnâ€™t help debug this. The only solution: manual comparison of working vs non-working environments.

---

## ğŸ’¡ Key Differences from Code Interpreter Agent

| Feature               | Code Interpreter Agent   | Local Function Agent             |
| --------------------- | ------------------------ | -------------------------------- |
| Execution Environment | Azure Compute (Remote)   | Local Laptop (Your Python Env)   |
| Tool Type             | Code Interpreter         | User-defined Functions           |
| Use Case              | Data Analysis, Plots     | Custom Actions, APIs, Automation |
| Setup Complexity      | Medium                   | Mediumâ€“Low                       |
| Debug Flexibility     | Medium (Cloud logs only) | High (Use breakpoints, logs)     |

---

## ğŸ Conclusion

You just built an **AI-powered bot** that:

- Talks to a hosted LLM in Azure
- Understands user intent
- Dynamically calls a real Python function on your machine
- Responds intelligently â€” combining AI smarts with local context

This architecture is perfect for:

- âœ‰ï¸ Smart email agents
- ğŸ  Home automation
- ğŸ® Game script assistants
- ğŸ” Local system control

And best of all â€” it's all real. No hallucination, no made-up answers. Just grounded, reliable **agentic intelligence**.
