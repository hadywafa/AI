# 🦙💻 Ultimate Guide: **LangChain + Ollama + Docker + Phi Model (Low Resource Setup)**

## 📦 What We’ll Cover

✅ What is Ollama  
✅ Why use Phi (and which one)  
✅ Full Docker Compose setup with GPU  
✅ Pulling & testing `phi` model  
✅ LangChain integration  
✅ Prompt templates, RAG, memory, deployment  
✅ Performance tips for low-spec systems  
✅ Bonus: LangServe + API routes

---

## 🧠 What is Ollama?

> **Ollama** is a local LLM runner. It’s like Docker but for models.

It wraps:

- Model binaries (GGUF format)
- Inference engine (`llama.cpp`)
- Configurations & REST API server

📍 Everything you need to run a model like `phi` or `llama2` locally, from a single CLI tool.

---

## 🌱 Why Use the `phi` Model?

> **Phi** by Microsoft is a tiny-yet-powerful open LLM built for efficiency.

| Model           | Size | Ideal for           | Memory Use                                       |
| --------------- | ---- | ------------------- | ------------------------------------------------ |
| `phi`           | 1.3B | General-purpose Q&A | ✅ Very Low                                      |
| `phi2`          | 2.7B | Better reasoning    | 🟡 Medium                                        |
| `phi3` (latest) | 4.8B | Strong code + tasks | 🔴 High (not for your device unless GPU is free) |

✅ We’ll use `phi` or `phi2` depending on your VRAM/RAM. On 16GB RAM with RTX 3050, `phi2` is usable too.

---

## 🐳 Step 1: Docker Compose Setup for Ollama

Here’s your full `docker-compose.yml` with persistent model volume + GPU passthrough:

```yaml
version: "3"
services:
  ollama:
    image: ollama/ollama
    ports:
      - "11434:11434"
    volumes:
      - C:\DockerVolumes\ollama:/root/.ollama

    # deploy:
    #   resources:
    #     reservations:
    #       devices:
    #         - capabilities: [gpu]
```

### 🧪 Run it

```bash
docker compose up -d
```

✅ You now have Ollama running and serving API at `http://localhost:11434`

---

## 📥 Step 2: Pull the `phi` model

You can do this inside the container or via API:

```bash
docker exec -it <container_name> ollama pull phi
```

💬 Example:

```ini
pulling manifest...
pulling model...
```

---

## 🔍 Step 3: Test the API Locally

From your host terminal:

```bash
curl http://localhost:11434/api/generate -d '{
  "model": "phi",
  "prompt": "Who was Nikola Tesla?"
}'
```

Expected output:

```ini
Nikola Tesla was a Serbian-American inventor and electrical engineer...
```

---

## 🧱 Step 4: Use LangChain with Ollama (`phi`)

### 🔧 Install Required Packages

```bash
pip install langchain langchain-ollama
```

### 🧠 Basic Example

```python
from langchain_ollama import OllamaLLM

llm = OllamaLLM(model="phi", base_url="http://localhost:11434")
response = llm.invoke("Explain how solar panels work.")
print(response)
```

---

## ✍️ Step 5: Prompt Templates

```python
from langchain.prompts import ChatPromptTemplate

template = ChatPromptTemplate.from_template("Q: {question}\nA:")
prompt = template.format_messages(question="What is quantum entanglement?")
llm = OllamaLLM(model="phi")

response = llm.invoke(prompt)
print(response)
```

---

## 🧠 Step 6: RAG (Retrieval-Augmented Generation)

Use your docs + local model for Q&A:

```python
from langchain_community.embeddings import OllamaEmbeddings
from langchain.vectorstores import FAISS
from langchain.chains import RetrievalQA

# Embed your documents
embeddings = OllamaEmbeddings(model="phi")
vectorstore = FAISS.from_texts(["LangChain is a framework for building with LLMs."], embedding=embeddings)
retriever = vectorstore.as_retriever()

# Build RAG chain
rag = RetrievalQA.from_chain_type(llm=OllamaLLM(model="phi"), retriever=retriever)
print(rag.run("What is LangChain?"))
```

---

## 💾 Step 7: Add Memory (Optional)

```python
from langchain.memory import ConversationBufferMemory
from langchain.chains import ConversationChain

memory = ConversationBufferMemory()
chat = ConversationChain(llm=OllamaLLM(model="phi"), memory=memory)

print(chat.run("Hi, I’m Hady."))
print(chat.run("What’s my name?"))
```

---

## 🚀 Step 8: Deploy with LangServe + FastAPI

Install:

```bash
pip install fastapi uvicorn langserve
```

Create `main.py`:

```python
from fastapi import FastAPI
from langchain_ollama import OllamaLLM
from langserve import add_routes

app = FastAPI()
llm = OllamaLLM(model="phi")
add_routes(app, llm)
```

Then run:

```bash
uvicorn main:app --reload
```

💻 Visit: [http://localhost:8000/docs](http://localhost:8000/docs)

---

## ⚡ Performance Tips for Low Spec Machines

| Tip                                      | Why                |
| ---------------------------------------- | ------------------ |
| Use `phi`, `phi2` only                   | Tiny footprint     |
| Run in quantized mode (default)          | Less VRAM          |
| Avoid long max_tokens                    | Saves RAM          |
| Use REST API vs LangChain in tight loops | Saves CPU overhead |
| Close background apps                    | Ollama eats RAM!   |

---

## 🔗 Bonus Tips

- To **list models** in Ollama:

  ```bash
  ollama list
  ```

- To **remove** a model:

  ```bash
  ollama rm phi
  ```

- To **debug GPU usage**, run inside container:

  ```bash
  nvidia-smi
  ```

---

## 📚 Resources

- 🔗 [Ollama Docs](https://ollama.com)
- 🔗 [LangChain Ollama Docs](https://python.langchain.com/docs/integrations/llms/ollama/)
- 🔗 [Ollama Model Library](https://ollama.com/library)
- 🧠 [LangChain GitHub](https://github.com/langchain-ai/langchain)

---

## 🧠 Final Thoughts

This setup makes your machine:

- A local ChatGPT server (no API key needed)
- A LangChain-ready agent framework
- RAG-ready & memory-capable
- Fast, cheap, and private

You're using **Ollama inside Docker**, **`phi` model for low spec**, and **LangChain for orchestration** — a 💯 pro move.

---

Let me know if you want:

- ✅ PDF export of this guide
- ✅ Ready GitHub repo template
- ✅ Custom UI for Ollama chat with Docker

🧙‍♂️ You’re now a **local LLM master** in training. Let's build magic.
