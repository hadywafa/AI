# ğŸ¦™ğŸ’» Ultimate Guide: **LangChain + Ollama + Docker + Phi Model (Low Resource Setup)**

## ğŸ“¦ What Weâ€™ll Cover

âœ… What is Ollama  
âœ… Why use Phi (and which one)  
âœ… Full Docker Compose setup with GPU  
âœ… Pulling & testing `phi` model  
âœ… LangChain integration  
âœ… Prompt templates, RAG, memory, deployment  
âœ… Performance tips for low-spec systems  
âœ… Bonus: LangServe + API routes

---

## ğŸ§  What is Ollama?

> **Ollama** is a local LLM runner. Itâ€™s like Docker but for models.

It wraps:

- Model binaries (GGUF format)
- Inference engine (`llama.cpp`)
- Configurations & REST API server

ğŸ“ Everything you need to run a model like `phi` or `llama2` locally, from a single CLI tool.

---

## ğŸŒ± Why Use the `phi` Model?

> **Phi** by Microsoft is a tiny-yet-powerful open LLM built for efficiency.

| Model           | Size | Ideal for           | Memory Use                                       |
| --------------- | ---- | ------------------- | ------------------------------------------------ |
| `phi`           | 1.3B | General-purpose Q&A | âœ… Very Low                                      |
| `phi2`          | 2.7B | Better reasoning    | ğŸŸ¡ Medium                                        |
| `phi3` (latest) | 4.8B | Strong code + tasks | ğŸ”´ High (not for your device unless GPU is free) |

âœ… Weâ€™ll use `phi` or `phi2` depending on your VRAM/RAM. On 16GB RAM with RTX 3050, `phi2` is usable too.

---

## ğŸ³ Step 1: Docker Compose Setup for Ollama

Hereâ€™s your full `docker-compose.yml` with persistent model volume + GPU passthrough:

```yaml
version: "3"
services:
  ollama:
    image: ollama/ollama
    ports:
      - "11434:11434"
    volumes:
      - C:\DockerVolumes\ollama:/root/.ollama

    # deploy:
    #   resources:
    #     reservations:
    #       devices:
    #         - capabilities: [gpu]
```

### ğŸ§ª Run it

```bash
docker compose up -d
```

âœ… You now have Ollama running and serving API at `http://localhost:11434`

---

## ğŸ“¥ Step 2: Pull the `phi` model

You can do this inside the container or via API:

```bash
docker exec -it <container_name> ollama pull phi
```

ğŸ’¬ Example:

```ini
pulling manifest...
pulling model...
```

---

## ğŸ” Step 3: Test the API Locally

From your host terminal:

```bash
curl http://localhost:11434/api/generate -d '{
  "model": "phi",
  "prompt": "Who was Nikola Tesla?"
}'
```

Expected output:

```ini
Nikola Tesla was a Serbian-American inventor and electrical engineer...
```

---

## ğŸ§± Step 4: Use LangChain with Ollama (`phi`)

### ğŸ”§ Install Required Packages

```bash
pip install langchain langchain-ollama
```

### ğŸ§  Basic Example

```python
from langchain_ollama import OllamaLLM

llm = OllamaLLM(model="phi", base_url="http://localhost:11434")
response = llm.invoke("Explain how solar panels work.")
print(response)
```

---

## âœï¸ Step 5: Prompt Templates

```python
from langchain.prompts import ChatPromptTemplate

template = ChatPromptTemplate.from_template("Q: {question}\nA:")
prompt = template.format_messages(question="What is quantum entanglement?")
llm = OllamaLLM(model="phi")

response = llm.invoke(prompt)
print(response)
```

---

## ğŸ§  Step 6: RAG (Retrieval-Augmented Generation)

Use your docs + local model for Q&A:

```python
from langchain_community.embeddings import OllamaEmbeddings
from langchain.vectorstores import FAISS
from langchain.chains import RetrievalQA

# Embed your documents
embeddings = OllamaEmbeddings(model="phi")
vectorstore = FAISS.from_texts(["LangChain is a framework for building with LLMs."], embedding=embeddings)
retriever = vectorstore.as_retriever()

# Build RAG chain
rag = RetrievalQA.from_chain_type(llm=OllamaLLM(model="phi"), retriever=retriever)
print(rag.run("What is LangChain?"))
```

---

## ğŸ’¾ Step 7: Add Memory (Optional)

```python
from langchain.memory import ConversationBufferMemory
from langchain.chains import ConversationChain

memory = ConversationBufferMemory()
chat = ConversationChain(llm=OllamaLLM(model="phi"), memory=memory)

print(chat.run("Hi, Iâ€™m Hady."))
print(chat.run("Whatâ€™s my name?"))
```

---

## ğŸš€ Step 8: Deploy with LangServe + FastAPI

Install:

```bash
pip install fastapi uvicorn langserve
```

Create `main.py`:

```python
from fastapi import FastAPI
from langchain_ollama import OllamaLLM
from langserve import add_routes

app = FastAPI()
llm = OllamaLLM(model="phi")
add_routes(app, llm)
```

Then run:

```bash
uvicorn main:app --reload
```

ğŸ’» Visit: [http://localhost:8000/docs](http://localhost:8000/docs)

---

## âš¡ Performance Tips for Low Spec Machines

| Tip                                      | Why                |
| ---------------------------------------- | ------------------ |
| Use `phi`, `phi2` only                   | Tiny footprint     |
| Run in quantized mode (default)          | Less VRAM          |
| Avoid long max_tokens                    | Saves RAM          |
| Use REST API vs LangChain in tight loops | Saves CPU overhead |
| Close background apps                    | Ollama eats RAM!   |

---

## ğŸ”— Bonus Tips

- To **list models** in Ollama:

  ```bash
  ollama list
  ```

- To **remove** a model:

  ```bash
  ollama rm phi
  ```

- To **debug GPU usage**, run inside container:

  ```bash
  nvidia-smi
  ```

---

## ğŸ“š Resources

- ğŸ”— [Ollama Docs](https://ollama.com)
- ğŸ”— [LangChain Ollama Docs](https://python.langchain.com/docs/integrations/llms/ollama/)
- ğŸ”— [Ollama Model Library](https://ollama.com/library)
- ğŸ§  [LangChain GitHub](https://github.com/langchain-ai/langchain)

---

## ğŸ§  Final Thoughts

This setup makes your machine:

- A local ChatGPT server (no API key needed)
- A LangChain-ready agent framework
- RAG-ready & memory-capable
- Fast, cheap, and private

You're using **Ollama inside Docker**, **`phi` model for low spec**, and **LangChain for orchestration** â€” a ğŸ’¯ pro move.

---

Let me know if you want:

- âœ… PDF export of this guide
- âœ… Ready GitHub repo template
- âœ… Custom UI for Ollama chat with Docker

ğŸ§™â€â™‚ï¸ Youâ€™re now a **local LLM master** in training. Let's build magic.
