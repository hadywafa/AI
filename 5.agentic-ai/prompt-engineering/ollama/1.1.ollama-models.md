# üß† Supported Models in Ollama

Ollama offers a diverse range of models that cater to various needs. Here's a curated list of some notable models:

- **Llama Series**:
  - `llama2`: Meta‚Äôs open-weight GPT-3.5-style model.
  - `llama3`: Latest iteration with improved performance.
- **Mistral**:
  - `mistral`: Lightweight, fast, and powerful 7B model.
- **Gemma**:
  - `gemma`: Google's lightweight open LLM.
- **Phi Series**:
  - `phi`: Microsoft's tiny 1.3B/2.7B models.
- **Code Llama**:
  - `codellama`: Specialized for code completion.
- **DeepSeek**:
  - `deepseek`: Advanced model for various tasks.
- **Others**:
  - `neural-chat`, `starling-lm`, `llava` (multimodal), and more.

For a comprehensive list and details, you can visit the [Ollama Model Library](https://ollama.com/library).

---

## üíª Hardware Compatibility & Limitations

### 1. **NVIDIA Quadro 2100M + 16GB RAM**

- **Compute Capability**: The Quadro 2100M has a compute capability of 2.1, which is below Ollama's minimum requirement of 5.0. Therefore, GPU acceleration isn't supported on this card.
- **CPU Usage**: You can still run models using the CPU, but performance will be significantly slower, especially for larger models.
- **Recommended Models**: Stick to smaller models like `phi` or `gemma` for a smoother experience.

### 2. **RTX 3050 + 16GB RAM**

- **Compute Capability**: The RTX 3050 boasts a compute capability of 8.6, making it fully compatible with Ollama.
- **Performance**: This setup allows for efficient GPU acceleration, enabling you to run medium to large models effectively.
- **Recommended Models**: You can comfortably run models like `llama2`, `mistral`, and even experiment with larger models, keeping an eye on VRAM usage.

---

## üìä Model Size vs. Hardware Requirements

Here's a general guideline to help you match models with your hardware:

| Model Size | VRAM Requirement | RAM Requirement | Notes                     |
| ---------- | ---------------- | --------------- | ------------------------- |
| 1B - 3B    | 4GB              | 8GB             | Suitable for most setups  |
| 7B         | 8GB              | 16GB            | Optimal on RTX 3050       |
| 13B        | 16GB             | 32GB            | May require optimization  |
| 33B+       | 24GB+            | 64GB+           | Demands high-end hardware |

_Note: These are approximate values. Actual requirements may vary based on specific model optimizations._

---

## üõ†Ô∏è Tips for Optimal Performance

- **Quantized Models**: Utilize quantized versions (e.g., `Q4_0`, `Q8_0`) to reduce VRAM usage without significantly compromising performance.
- **Model Selection**: Choose models that align with your hardware capabilities to ensure smooth operation.
- **Monitoring**: Keep an eye on system resources during model execution to prevent bottlenecks.

---

If you need assistance setting up a specific model or have further questions about optimizing performance on your hardware, feel free to ask!
