# 🐳🦙 **Run Ollama + Gemma in Docker** – Full Tutorial

✅ This guide will help you:

- Run Ollama in a Docker container
- Load and serve the **Gemma** model
- Interact with the model via terminal or API
- Optional: Use with LangChain or REST client

---

## 📦 Step 0: Prerequisites

| Tool                    | Required Version                       |
| ----------------------- | -------------------------------------- |
| Docker Engine           | ✅ Installed + running                 |
| NVIDIA GPU Driver       | ✅ Latest for RTX 3050                 |
| `nvidia-docker` runtime | ✅ Needed for GPU passthrough (Linux)  |
| Ollama account          | ❌ Not needed (no cloud auth required) |

---

## 🐳 Step 1: Pull the Ollama Docker Image

```bash
docker pull ollama/ollama
```

This will get the latest official Ollama runtime image.

---

## ⚙️ Step 2: Start Ollama in Docker (GPU-Ready)

### 🧠 Option 1: For **Linux with NVIDIA GPU**

```bash
docker run --rm -it \
  --gpus all \
  -p 11434:11434 \
  -v ollama:/root/.ollama \
  ollama/ollama
```

This:

- Enables GPU access
- Maps the port for API use
- Creates a persistent volume to cache downloaded models

### 🧠 Option 2: For **Mac/Windows (no GPU)**

```bash
docker run --rm -it \
  -p 11434:11434 \
  -v ollama:/root/.ollama \
  ollama/ollama
```

⚠️ Performance will be **CPU-only**, but still works.

---

## 📥 Step 3: Download & Run the Gemma Model

Once inside the container shell, run:

```bash
ollama run gemma
```

Ollama will:

- Pull the `gemma` model (around ~2–3 GB)
- Start a local REPL interface
- Let you chat directly

🧪 Try this:

```ini
> What is the capital of Egypt?
Cairo is the capital and largest city of Egypt.
```

🎉 That’s it — you’re running Gemma locally in Docker!

---

## 🧪 Step 4: (Optional) Use Ollama API from Host

The container exposes `localhost:11434` on your **host machine**, so from your terminal or Python script:

```bash
curl http://localhost:11434/api/generate -d '{
  "model": "gemma",
  "prompt": "Who is Alan Turing?"
}'
```

Or in Python:

```python
import requests

res = requests.post("http://localhost:11434/api/generate", json={
    "model": "gemma",
    "prompt": "Explain black holes in simple terms."
})

print(res.json()['response'])
```

---

## 🧠 Step 5: Make Ollama Persistent (Optional Docker Compose)

Here’s a `docker-compose.yml` you can use:

```yaml
version: "3"
services:
  ollama:
    image: ollama/ollama
    ports:
      - "11434:11434"
    volumes:
      - ollama_data:/root/.ollama
    deploy:
      resources:
        reservations:
          devices:
            - capabilities: [gpu]

volumes:
  ollama_data:
```

Start it:

```bash
docker compose up
```

🧠 Now you can keep your models downloaded and Ollama always running.

---

## 💡 Bonus: Use with LangChain

```python
from langchain.llms import Ollama

llm = Ollama(model="gemma", base_url="http://localhost:11434")
response = llm.invoke("What is quantum computing?")
print(response)
```

Boom — LangChain agent powered by **local Gemma + Docker**. 🔥

---

## 🧠 Summary

| Step | What You Did                |
| ---- | --------------------------- |
| ✅ 1 | Pulled Ollama Docker image  |
| ✅ 2 | Ran it with GPU or CPU mode |
| ✅ 3 | Pulled and used Gemma       |
| ✅ 4 | Called the API              |
| ✅ 5 | (Optional) Used LangChain   |

---

Would you like me to:

- Build this as a **Colab emulator** to simulate?
- Package it into a **ready GitHub repo with Docker + API + LangChain demo**?

Say the word and I’ll generate it 🚀💻
