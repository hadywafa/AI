# ðŸ³ðŸ¦™ **Run Ollama + Gemma in Docker** â€“ Full Tutorial

âœ… This guide will help you:

- Run Ollama in a Docker container
- Load and serve the **Gemma** model
- Interact with the model via terminal or API
- Optional: Use with LangChain or REST client

---

## ðŸ“¦ Step 0: Prerequisites

| Tool                    | Required Version                       |
| ----------------------- | -------------------------------------- |
| Docker Engine           | âœ… Installed + running                 |
| NVIDIA GPU Driver       | âœ… Latest for RTX 3050                 |
| `nvidia-docker` runtime | âœ… Needed for GPU passthrough (Linux)  |
| Ollama account          | âŒ Not needed (no cloud auth required) |

---

## ðŸ³ Step 1: Pull the Ollama Docker Image

```bash
docker pull ollama/ollama
```

This will get the latest official Ollama runtime image.

---

## âš™ï¸ Step 2: Start Ollama in Docker (GPU-Ready)

### ðŸ§  Option 1: For **Linux with NVIDIA GPU**

```bash
docker run --rm -it \
  --gpus all \
  -p 11434:11434 \
  -v ollama:/root/.ollama \
  ollama/ollama
```

This:

- Enables GPU access
- Maps the port for API use
- Creates a persistent volume to cache downloaded models

### ðŸ§  Option 2: For **Mac/Windows (no GPU)**

```bash
docker run --rm -it \
  -p 11434:11434 \
  -v ollama:/root/.ollama \
  ollama/ollama
```

âš ï¸ Performance will be **CPU-only**, but still works.

---

## ðŸ“¥ Step 3: Download & Run the Gemma Model

Once inside the container shell, run:

```bash
ollama run gemma
```

Ollama will:

- Pull the `gemma` model (around ~2â€“3 GB)
- Start a local REPL interface
- Let you chat directly

ðŸ§ª Try this:

```ini
> What is the capital of Egypt?
Cairo is the capital and largest city of Egypt.
```

ðŸŽ‰ Thatâ€™s it â€” youâ€™re running Gemma locally in Docker!

---

## ðŸ§ª Step 4: (Optional) Use Ollama API from Host

The container exposes `localhost:11434` on your **host machine**, so from your terminal or Python script:

```bash
curl http://localhost:11434/api/generate -d '{
  "model": "gemma",
  "prompt": "Who is Alan Turing?"
}'
```

Or in Python:

```python
import requests

res = requests.post("http://localhost:11434/api/generate", json={
    "model": "gemma",
    "prompt": "Explain black holes in simple terms."
})

print(res.json()['response'])
```

---

## ðŸ§  Step 5: Make Ollama Persistent (Optional Docker Compose)

Hereâ€™s a `docker-compose.yml` you can use:

```yaml
version: "3"
services:
  ollama:
    image: ollama/ollama
    ports:
      - "11434:11434"
    volumes:
      - ollama_data:/root/.ollama
    deploy:
      resources:
        reservations:
          devices:
            - capabilities: [gpu]

volumes:
  ollama_data:
```

Start it:

```bash
docker compose up
```

ðŸ§  Now you can keep your models downloaded and Ollama always running.

---

## ðŸ’¡ Bonus: Use with LangChain

```python
from langchain.llms import Ollama

llm = Ollama(model="gemma", base_url="http://localhost:11434")
response = llm.invoke("What is quantum computing?")
print(response)
```

Boom â€” LangChain agent powered by **local Gemma + Docker**. ðŸ”¥

---

## ðŸ§  Summary

| Step | What You Did                |
| ---- | --------------------------- |
| âœ… 1 | Pulled Ollama Docker image  |
| âœ… 2 | Ran it with GPU or CPU mode |
| âœ… 3 | Pulled and used Gemma       |
| âœ… 4 | Called the API              |
| âœ… 5 | (Optional) Used LangChain   |

---

Would you like me to:

- Build this as a **Colab emulator** to simulate?
- Package it into a **ready GitHub repo with Docker + API + LangChain demo**?

Say the word and Iâ€™ll generate it ðŸš€ðŸ’»
