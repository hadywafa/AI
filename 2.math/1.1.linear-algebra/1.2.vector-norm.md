# 📏 Vector Norms & Normalization

> _“Because sometimes you just need to know how long your vector is… and how to tame it.”_

---

## 🤔 What Is a Vector Norm?

### 📘 Official Definition

A **vector norm** is a function that gives a **non-negative length (or size)** of a vector in a vector space.

🧠 In simple terms:  
It tells us **how long the vector is** — like measuring the length of an arrow.

---

## 🚶‍♂️ Real-Life Analogy

Imagine you walk from point A to point B in a straight line.  
The **norm** of that walk is the **total straight-line distance you walked** — no turns, no curves.

---

## 🧮 Common Types of Norms (With Examples)

---

### ✅ 1. **L2 Norm** – Euclidean Norm

📐 This is the most common one. It's the regular distance — like measuring with a ruler.

**Formula:**

$$
\|v\|_2 = \sqrt{v_1^2 + v_2^2 + \dots + v_n^2}
$$

**Example:**

Let:

```text
v = [3, 4]
```

Then:

$$
\|v\|_2 = \sqrt{3^2 + 4^2} = \sqrt{9 + 16} = \sqrt{25} = 5
$$

✅ This is the **length** of the vector from the origin to the point (3, 4).

---

### ✅ 2. **L1 Norm** – Manhattan (Taxicab) Norm

🚕 This measures the distance like a taxi driving on a grid — it adds up all steps in all directions.

**Formula:**

$$
\|v\|_1 = |v_1| + |v_2| + \dots + |v_n|
$$

**Example:**

```text
v = [3, -4]
```

Then:

$$
\|v\|_1 = |3| + |-4| = 3 + 4 = 7
$$

✅ It counts total blocks walked — not the shortcut.

---

### ✅ 3. **L∞ Norm** – Max (Infinity) Norm

🚨 This one just asks: “Which component is the biggest (in absolute value)?”

**Formula:**

$$
\|v\|_\infty = \max\left(|v_1|, |v_2|, \dots, |v_n|\right)
$$

**Example:**

```text
v = [3, -4, 2]
```

Then:

$$
\|v\|_\infty = \max(3, 4, 2) = 4
$$

✅ Used when you care about the **worst-case** (biggest step).

---

## 🎯 Why Are Norms Important in AI/ML?

| Use Case                 | Why It Matters                             |
| ------------------------ | ------------------------------------------ |
| **Distance Calculation** | Needed for algorithms like KNN, clustering |
| **Loss Functions**       | L1 and L2 norms power regression models    |
| **Regularization**       | Helps prevent overfitting in ML            |
| **Gradient Descent**     | Step size can depend on norm of gradients  |
| **Normalization**        | Models learn faster and better with norms  |

---

## 🔁 What Is Normalization?

### 📘 Official Definition

**Normalization** is the process of rescaling a vector so that its **length becomes 1**.

This gives us a **unit vector** (same direction, but smaller or stretched to length 1).

---

### 💡 Why Normalize?

✅ To make all vectors equal in scale  
✅ Helps with learning algorithms  
✅ Removes bias from features with large values

---

### 🧪 Formula for Normalization

Given a vector $v$, the normalized version is:

$$
\hat{v} = \frac{v}{\|v\|}
$$

You divide **every component** by the vector's norm.

---

### 💥 Example: Normalize a Vector

Let:

```text
v = [3, 4]
```

#### 1. First, compute the L2 norm

$$
\|v\| = \sqrt{3^2 + 4^2} = 5
$$

#### 2. Now normalize

$$
\hat{v} = \left[\frac{3}{5}, \frac{4}{5}\right] = [0.6, 0.8]
$$

#### 3. Double-check the length of the new vector

$$
\|\hat{v}\| = \sqrt{0.6^2 + 0.8^2} = \sqrt{0.36 + 0.64} = \sqrt{1} = 1
$$

✅ It worked! The normalized vector has **length = 1**.

---

## 🧠 TL;DR – Norms vs Normalization

| Term              | What It Means                       |
| ----------------- | ----------------------------------- |
| **Norm**          | The length or size of a vector      |
| **L2 Norm**       | Euclidean (straight-line) distance  |
| **L1 Norm**       | Sum of absolute values (grid walk)  |
| **L∞ Norm**       | Largest single component            |
| **Normalization** | Rescale a vector to have length = 1 |

---

## 🧪 Mini Challenge 🎯

You’re given:

```text
a = [6, 8]
```

1. Find the **L2 norm**
2. Normalize the vector

👉 Try it yourself, then use this to check:

### ✅ Answe

#### 1. Compute the norm

$$
\|a\| = \sqrt{6^2 + 8^2} = \sqrt{36 + 64} = \sqrt{100} = 10
$$

#### 2. Normalize

$$
\hat{a} = \left[\frac{6}{10}, \frac{8}{10}\right] = [0.6, 0.8]
$$
